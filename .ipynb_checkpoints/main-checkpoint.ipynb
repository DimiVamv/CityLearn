{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "\n",
    "from gym import core, spaces\n",
    "\n",
    "import os\n",
    "import ptan\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import model, common\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentD4PG(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent implementing noisy agent\n",
    "    \"\"\"\n",
    "    def __init__(self, net, device=\"cpu\", epsilon=1.0):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, states, agent_states):\n",
    "        states_v = ptan.agent.float32_preprocessor(states).to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "        actions += self.epsilon * np.random.normal(size=actions.shape)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions, agent_states\n",
    "    \n",
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(8 + act_size, 6),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = [4, 5, 9, 16, 21, 26, 33, 36, 49, 59]\n",
    "# demand_file = r'C:\\Users\\jrv966\\Documents\\GitHub\\multi-agent-RL\\simulationData\\AustinResidential_TH.csv'\n",
    "# weather_file = r'C:\\Users\\jrv966\\Documents\\GitHub\\multi-agent-RL\\simulationData\\Austin_Airp_TX-hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = {}\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings[uid] = Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid])\n",
    "    buildings[uid].state_action_space(np.array([24.0, 40.0, 1.001]), np.array([1.0, 17.0, -0.001]), np.array([0.5]), np.array([-0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_loader(demand_file, weather_file, buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearn(demand_file, weather_file, buildings = buildings, time_resolution = 1, simulation_period = (3500,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    N_AGENTS = 2\n",
    "    GAMMA = 0.99\n",
    "    BATCH_SIZE = 5000\n",
    "    LEARNING_RATE_ACTOR = 1e-4\n",
    "    LEARNING_RATE_CRITIC = 1e-3\n",
    "    REPLAY_SIZE = 5000\n",
    "    REPLAY_INITIAL = 100\n",
    "    TEST_ITERS = 120\n",
    "    EPSILON_DECAY_LAST_FRAME = 1000\n",
    "    EPSILON_START = 1.2\n",
    "    EPSILON_FINAL = 0.02\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    act_net, crt_net, tgt_act_net, tgt_crt_net, agent, exp_source, buffer, act_opt, crt_opt, frame_idx = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    rew_last_1000, rew, track_loss_critic, track_loss_actor = {}, {}, {}, {}\n",
    "    env.reset()\n",
    "    for uid in building_ids:\n",
    "        env(uid)\n",
    "        #Create as many actor and critic nets as number of agents\n",
    "        #Actor: states_agent_i -> actions_agent_i\n",
    "        act_net[uid] = DDPGActor(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        #Critic: states_all_agents + actions_all_agents -> Q-value_agent_i [1]\n",
    "        crt_net[uid] = DDPGCritic(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        tgt_act_net[uid] = ptan.agent.TargetNet(act_net[uid])\n",
    "        tgt_crt_net[uid] = ptan.agent.TargetNet(crt_net[uid])\n",
    "\n",
    "        agent[uid] = model.AgentD4PG(act_net[uid], device=device)\n",
    "        exp_source[uid] = ptan.experience.ExperienceSourceFirstLast(env, agent[uid], gamma=GAMMA, steps_count=1)\n",
    "        buffer[uid] = ptan.experience.ExperienceReplayBuffer(exp_source[uid], buffer_size=REPLAY_SIZE)\n",
    "        act_opt[uid] = optim.Adam(act_net[uid].parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "        crt_opt[uid] = optim.Adam(crt_net[uid].parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "\n",
    "        frame_idx[uid] = 0\n",
    "\n",
    "        rew_last_1000[uid], rew[uid], track_loss_critic[uid], track_loss_actor[uid] = [], [], [], []\n",
    "\n",
    "    batch, states_v, actions_v, rewards_v, dones_mask, last_states_v, q_v, last_act_v, q_last_v, q_ref_v, critic_loss_v, cur_actions_v, actor_loss_v = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    while not env._terminal():\n",
    "        if frame_idx[4]%100 == 0:\n",
    "            print(frame_idx[uid])\n",
    "        for uid in buildings:\n",
    "            env(uid)\n",
    "            agent[uid].epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx[uid] / EPSILON_DECAY_LAST_FRAME)\n",
    "            frame_idx[uid] += 1           \n",
    "            buffer[uid].populate(1)\n",
    "        \n",
    "        price = env.total_electric_consumption[-1]*3e-5 + 0.045\n",
    "        for uid in buildings:  \n",
    "            env(uid)  \n",
    "            electricity_cost = buffer[uid].buffer[-1].reward*price\n",
    "            buffer[uid].buffer[-1] = buffer[uid].buffer[-1]._replace(reward=electricity_cost)\n",
    "\n",
    "        if len(buffer[uid]) < REPLAY_INITIAL:\n",
    "            continue   \n",
    "\n",
    "        for uid in buildings:\n",
    "            env(uid)\n",
    "            for k in range(6):\n",
    "                batch[uid] = buffer[uid].sample(BATCH_SIZE)\n",
    "                states_v[uid], actions_v[uid], rewards_v[uid], dones_mask[uid], last_states_v[uid] = common.unpack_batch_ddqn(batch[uid], device) \n",
    "\n",
    "                # TRAIN CRITIC\n",
    "                crt_opt[uid].zero_grad()\n",
    "                #Obtaining Q' using critic net with parameters teta_Q'\n",
    "                q_v[uid] = crt_net[uid](states_v[uid], actions_v[uid])\n",
    "\n",
    "                #Obtaining estimated optimal actions a|teta_mu from target actor net and from s_i+1.\n",
    "                last_act_v[uid] = tgt_act_net[uid].target_model(last_states_v[uid]) #<----- Actor to train Critic\n",
    "\n",
    "                #Obtaining Q'(s_i+1, a|teta_mu) from critic net Q'\n",
    "                q_last_v[uid] = tgt_crt_net[uid].target_model(last_states_v[uid], last_act_v[uid])\n",
    "                q_last_v[uid][dones_mask[uid]] = 0.0\n",
    "\n",
    "                #Q_target used to train critic net Q'\n",
    "                q_ref_v[uid] = rewards_v[uid].unsqueeze(dim=-1) + q_last_v[uid] * GAMMA\n",
    "                critic_loss_v[uid] = F.mse_loss(q_v[uid], q_ref_v[uid].detach())\n",
    "                critic_loss_v[uid].backward()\n",
    "                crt_opt[uid].step()\n",
    "\n",
    "                # TRAIN ACTOR\n",
    "                act_opt[uid].zero_grad()\n",
    "                #Obtaining estimated optimal current actions a|teta_mu from actor net and from s_i\n",
    "                cur_actions_v[uid] = act_net[uid](states_v[uid])\n",
    "\n",
    "                #Actor loss = mean{ -Q_i'(s_i, a|teta_mu) }\n",
    "                actor_loss_v[uid] = -crt_net[uid](states_v[uid], cur_actions_v[uid]) #<----- Critic to train Actor\n",
    "                actor_loss_v[uid] = actor_loss_v[uid].mean()\n",
    "                #Find gradient of the loss and backpropagate to perform the updates of teta_mu\n",
    "                actor_loss_v[uid].backward()\n",
    "                act_opt[uid].step()\n",
    "\n",
    "                if frame_idx[uid] % 1 == 0:\n",
    "                    tgt_act_net[uid].alpha_sync(alpha=1 - 0.1)\n",
    "                    tgt_crt_net[uid].alpha_sync(alpha=1 - 0.1)\n",
    "\n",
    "        env.next_hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = {}\n",
    "for uid in buildings:\n",
    "    rew[uid] = env.total_electric_consumption\n",
    "filename_results = 'CityLearn_DDPG.csv'\n",
    "results = pd.DataFrame.from_dict(rew)\n",
    "results.to_csv(filename_results,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in buildings:\n",
    "    plt.plot(range(len(env.action_track[uid])), env.action_track[uid])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in buildings:\n",
    "    plt.plot(range(len(env.action_track[uid])), env.action_track[uid])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
