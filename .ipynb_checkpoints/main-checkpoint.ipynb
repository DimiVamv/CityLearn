{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "\n",
    "from gym import core, spaces\n",
    "\n",
    "import os\n",
    "import ptan\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import model, common\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentD4PG(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent implementing noisy agent\n",
    "    \"\"\"\n",
    "    def __init__(self, net, device=\"cpu\", epsilon=1.0):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, states, agent_states):\n",
    "        states_v = ptan.agent.float32_preprocessor(states).to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "        actions += self.epsilon * np.random.normal(size=actions.shape)\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions, agent_states\n",
    "    \n",
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(8 + act_size, 6),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = [4, 5, 9]#, 16, 21, 26, 33, 36, 49, 59]\n",
    "# demand_file = r'C:\\Users\\jrv966\\Documents\\GitHub\\multi-agent-RL\\simulationData\\AustinResidential_TH.csv'\n",
    "# weather_file = r'C:\\Users\\jrv966\\Documents\\GitHub\\multi-agent-RL\\simulationData\\Austin_Airp_TX-hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = {}\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings[uid] = Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid])\n",
    "    buildings[uid].state_action_space(np.array([24.0, 40.0, 1.001]), np.array([1.0, 17.0, -0.001]), np.array([0.5]), np.array([-0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_loader(demand_file, weather_file, buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {}\n",
    "for uid in building_ids:\n",
    "    env[uid] = CityLearn(demand_file, weather_file, buildings = {uid: buildings[uid]}, time_resolution = 1, simulation_period = (3500,6000))\n",
    "    env[uid](uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env[5].buildings[5].time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.        , 19.        ,  0.99604167]),\n",
       " -20552.882158618435,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env[5].step(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    N_AGENTS = 2\n",
    "    GAMMA = 0.99\n",
    "    BATCH_SIZE = 5000\n",
    "    LEARNING_RATE_ACTOR = 1e-4\n",
    "    LEARNING_RATE_CRITIC = 1e-3\n",
    "    REPLAY_SIZE = 5000\n",
    "    REPLAY_INITIAL = 100\n",
    "    TEST_ITERS = 120\n",
    "    EPSILON_DECAY_LAST_FRAME = 1000\n",
    "    EPSILON_START = 1.2\n",
    "    EPSILON_FINAL = 0.02\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    act_net, crt_net, tgt_act_net, tgt_crt_net, agent, exp_source, buffer, act_opt, crt_opt, frame_idx = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    rew_last_1000, rew, track_loss_critic, track_loss_actor = {}, {}, {}, {}\n",
    "    for uid in buildings:\n",
    "        env[uid].reset()\n",
    "    for uid in building_ids:\n",
    "        #Create as many actor and critic nets as number of agents\n",
    "        #Actor: states_agent_i -> actions_agent_i\n",
    "        act_net[uid] = DDPGActor(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        #Critic: states_all_agents + actions_all_agents -> Q-value_agent_i [1]\n",
    "        crt_net[uid] = DDPGCritic(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        tgt_act_net[uid] = ptan.agent.TargetNet(act_net[uid])\n",
    "        tgt_crt_net[uid] = ptan.agent.TargetNet(crt_net[uid])\n",
    "\n",
    "        agent[uid] = model.AgentD4PG(act_net[uid], device=device)\n",
    "        exp_source[uid] = ptan.experience.ExperienceSourceFirstLast(env[uid], agent[uid], gamma=GAMMA, steps_count=1)\n",
    "        buffer[uid] = ptan.experience.ExperienceReplayBuffer(exp_source[uid], buffer_size=REPLAY_SIZE)\n",
    "        act_opt[uid] = optim.Adam(act_net[uid].parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "        crt_opt[uid] = optim.Adam(crt_net[uid].parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "\n",
    "        frame_idx[uid] = 0\n",
    "\n",
    "        rew_last_1000[uid], rew[uid], track_loss_critic[uid], track_loss_actor[uid] = [], [], [], []\n",
    "\n",
    "    batch, states_v, actions_v, rewards_v, dones_mask, last_states_v, q_v, last_act_v, q_last_v, q_ref_v, critic_loss_v, cur_actions_v, actor_loss_v = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    cost, price_list, buffer_reward = {},{},{}\n",
    "    for uid in buildings:\n",
    "        cost[uid] = []\n",
    "        price_list[uid] = []\n",
    "        buffer_reward[uid] = []\n",
    "    while not env[building_ids[-1]]._terminal():\n",
    "        if frame_idx[4]%100 == 0:\n",
    "            print(frame_idx[uid])\n",
    "        for uid in buildings:\n",
    "#             print(env[uid].buildings[uid].time_step)\n",
    "            agent[uid].epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx[uid] / EPSILON_DECAY_LAST_FRAME)\n",
    "            frame_idx[uid] += 1           \n",
    "            buffer[uid].populate(1)\n",
    "#             print(buffer[uid].buffer[-1])\n",
    "#             print(env[uid].buildings[uid].time_step)\n",
    "            \n",
    "        \n",
    "        price = env[uid].total_electric_consumption[-1]*3e-5 + 0.045\n",
    "        price_list[uid].append(price)\n",
    "        for uid in buildings:  \n",
    "            buffer_reward[uid].append(buffer[uid].buffer[-1].reward)\n",
    "            electricity_cost = buffer[uid].buffer[-1].reward*price\n",
    "            cost[uid].append(-electricity_cost)\n",
    "            buffer[uid].buffer[-1] = buffer[uid].buffer[-1]._replace(reward=electricity_cost)\n",
    "\n",
    "        if len(buffer[uid]) < REPLAY_INITIAL:\n",
    "            continue   \n",
    "\n",
    "        for uid in buildings:\n",
    "            for k in range(6):\n",
    "                batch[uid] = buffer[uid].sample(BATCH_SIZE)\n",
    "                states_v[uid], actions_v[uid], rewards_v[uid], dones_mask[uid], last_states_v[uid] = common.unpack_batch_ddqn(batch[uid], device) \n",
    "\n",
    "                # TRAIN CRITIC\n",
    "                crt_opt[uid].zero_grad()\n",
    "                #Obtaining Q' using critic net with parameters teta_Q'\n",
    "                q_v[uid] = crt_net[uid](states_v[uid], actions_v[uid])\n",
    "\n",
    "                #Obtaining estimated optimal actions a|teta_mu from target actor net and from s_i+1.\n",
    "                last_act_v[uid] = tgt_act_net[uid].target_model(last_states_v[uid]) #<----- Actor to train Critic\n",
    "\n",
    "                #Obtaining Q'(s_i+1, a|teta_mu) from critic net Q'\n",
    "                q_last_v[uid] = tgt_crt_net[uid].target_model(last_states_v[uid], last_act_v[uid])\n",
    "                q_last_v[uid][dones_mask[uid]] = 0.0\n",
    "\n",
    "                #Q_target used to train critic net Q'\n",
    "                q_ref_v[uid] = rewards_v[uid].unsqueeze(dim=-1) + q_last_v[uid] * GAMMA\n",
    "                critic_loss_v[uid] = F.mse_loss(q_v[uid], q_ref_v[uid].detach())\n",
    "                critic_loss_v[uid].backward()\n",
    "                crt_opt[uid].step()\n",
    "\n",
    "                # TRAIN ACTOR\n",
    "                act_opt[uid].zero_grad()\n",
    "                #Obtaining estimated optimal current actions a|teta_mu from actor net and from s_i\n",
    "                cur_actions_v[uid] = act_net[uid](states_v[uid])\n",
    "\n",
    "                #Actor loss = mean{ -Q_i'(s_i, a|teta_mu) }\n",
    "                actor_loss_v[uid] = -crt_net[uid](states_v[uid], cur_actions_v[uid]) #<----- Critic to train Actor\n",
    "                actor_loss_v[uid] = actor_loss_v[uid].mean()\n",
    "                #Find gradient of the loss and backpropagate to perform the updates of teta_mu\n",
    "                actor_loss_v[uid].backward()\n",
    "                act_opt[uid].step()\n",
    "\n",
    "                if frame_idx[uid] % 1 == 0:\n",
    "                    tgt_act_net[uid].alpha_sync(alpha=1 - 0.1)\n",
    "                    tgt_crt_net[uid].alpha_sync(alpha=1 - 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExperienceFirstLast(state=array([ 8. , 25.5,  0. ]), action=array([-0.9956575], dtype=float32), reward=0.0, last_state=array([ 9. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.4,  0. ]), action=array([-0.9886107], dtype=float32), reward=0.0, last_state=array([11. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.9,  0. ]), action=array([-0.98092914], dtype=float32), reward=0.0, last_state=array([13. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 34.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 35.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.2,  0. ]), action=array([-0.9897137], dtype=float32), reward=0.0, last_state=array([15. , 35.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 35.8,  0. ]), action=array([-0.9889407], dtype=float32), reward=0.0, last_state=array([16. , 35.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 35.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 35.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.7,  0. ]), action=array([-0.9797813], dtype=float32), reward=0.0, last_state=array([19. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.4,  0. ]), action=array([-0.9762292], dtype=float32), reward=0.0, last_state=array([20. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.9,  0. ]), action=array([-0.99499506], dtype=float32), reward=0.0, last_state=array([21. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.2,  0. ]), action=array([-0.97166574], dtype=float32), reward=0.0, last_state=array([24., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 27.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.3,  0. ]), action=array([-0.9894214], dtype=float32), reward=0.0, last_state=array([ 2. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 25.1,  0. ]), action=array([-0.9855229], dtype=float32), reward=0.0, last_state=array([ 4. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.7,  0. ]), action=array([-0.98205924], dtype=float32), reward=0.0, last_state=array([ 5. , 24.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 24.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.3,  0. ]), action=array([-0.99125826], dtype=float32), reward=0.0, last_state=array([ 7. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 25.2,  0. ]), action=array([-0.97909373], dtype=float32), reward=0.0, last_state=array([ 8. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.9,  0. ]), action=array([-0.99148846], dtype=float32), reward=0.0, last_state=array([ 9. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.9,  0. ]), action=array([-0.9812093], dtype=float32), reward=0.0, last_state=array([10. , 30.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.8,  0. ]), action=array([-0.9914923], dtype=float32), reward=0.0, last_state=array([12. , 34.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.4,  0. ]), action=array([-0.993307], dtype=float32), reward=0.0, last_state=array([13. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.7,  0. ]), action=array([-0.9996606], dtype=float32), reward=0.0, last_state=array([14. , 36.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.7,  0. ]), action=array([-0.9941658], dtype=float32), reward=0.0, last_state=array([15. , 37.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 37.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 37.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 36.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 36.5,  0. ]), action=array([-0.97456867], dtype=float32), reward=0.0, last_state=array([19. , 35.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 35.2,  0. ]), action=array([-0.98236847], dtype=float32), reward=0.0, last_state=array([20. , 33.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 33.8,  0. ]), action=array([-0.99914813], dtype=float32), reward=0.0, last_state=array([21. , 32.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 32.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.7,  0. ]), action=array([-0.9817885], dtype=float32), reward=0.0, last_state=array([23. , 29.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 29.2,  0. ]), action=array([-0.980562], dtype=float32), reward=0.0, last_state=array([24. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.6,  0. ]), action=array([-0.9748949], dtype=float32), reward=0.0, last_state=array([ 1. , 26.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.4,  0. ]), action=array([-0.9990215], dtype=float32), reward=0.0, last_state=array([ 2., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 25.,  0.]), action=array([-0.9816762], dtype=float32), reward=0.0, last_state=array([ 3. , 24.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.2,  0. ]), action=array([-0.98133], dtype=float32), reward=0.0, last_state=array([ 4. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.5,  0. ]), action=array([-0.9945416], dtype=float32), reward=0.0, last_state=array([ 5. , 22.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.9,  0. ]), action=array([-0.98959285], dtype=float32), reward=0.0, last_state=array([ 6. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.8,  0. ]), action=array([-0.9574135], dtype=float32), reward=0.0, last_state=array([ 7. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 26.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 28.2,  0. ]), action=array([-0.989453], dtype=float32), reward=0.0, last_state=array([11. , 29.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 29.7,  0. ]), action=array([-0.9896888], dtype=float32), reward=0.0, last_state=array([12. , 31.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 31.1,  0. ]), action=array([-0.96903443], dtype=float32), reward=0.0, last_state=array([13. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 32.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 32.9,  0. ]), action=array([-0.98446304], dtype=float32), reward=0.0, last_state=array([15. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 33.5,  0. ]), action=array([-0.9822041], dtype=float32), reward=0.0, last_state=array([17. , 33.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 33.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 32.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 32.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 31.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 30.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 29.,  0.]), action=array([-0.97331715], dtype=float32), reward=0.0, last_state=array([22., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([22., 28.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 27.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 26.,  0.]), action=array([-0.9751776], dtype=float32), reward=0.0, last_state=array([ 1. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.2,  0. ]), action=array([-0.99486196], dtype=float32), reward=0.0, last_state=array([ 3. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.9,  0. ]), action=array([-0.9880548], dtype=float32), reward=0.0, last_state=array([ 4. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 24.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 24.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.5,  0. ]), action=array([-0.9858341], dtype=float32), reward=0.0, last_state=array([ 7. , 25.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 25.4,  0. ]), action=array([-0.9713815], dtype=float32), reward=0.0, last_state=array([ 8. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.8,  0. ]), action=array([-0.9941013], dtype=float32), reward=0.0, last_state=array([ 9. , 28.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.3,  0. ]), action=array([-0.9843869], dtype=float32), reward=0.0, last_state=array([11. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 34.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 34.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 35.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 35.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 35.7,  0. ]), action=array([-0.9882283], dtype=float32), reward=0.0, last_state=array([17. , 35.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.4,  0. ]), action=array([-0.987423], dtype=float32), reward=0.0, last_state=array([18. , 34.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.4,  0. ]), action=array([-0.98378986], dtype=float32), reward=0.0, last_state=array([21. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 30.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.4,  0. ]), action=array([-0.98702276], dtype=float32), reward=0.0, last_state=array([23. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 29.3,  0. ]), action=array([-0.9837598], dtype=float32), reward=0.0, last_state=array([24. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 28.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 27.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 26.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 26.6,  0. ]), action=array([-0.98371994], dtype=float32), reward=0.0, last_state=array([ 3., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 3., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 25.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 25.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.7,  0. ]), action=array([-0.98150736], dtype=float32), reward=0.0, last_state=array([ 7. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 25.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.7,  0. ]), action=array([-0.9658761], dtype=float32), reward=0.0, last_state=array([ 9. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 27.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 27.2,  0. ]), action=array([-0.9931469], dtype=float32), reward=0.0, last_state=array([12., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([12., 28.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 29.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 30.8,  0. ]), action=array([-0.9827335], dtype=float32), reward=0.0, last_state=array([15., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([15., 32.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 32.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 32.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 32.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 32.5,  0. ]), action=array([-0.9966379], dtype=float32), reward=0.0, last_state=array([18. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.4,  0. ]), action=array([-0.9905058], dtype=float32), reward=0.0, last_state=array([21. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([22., 29.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 27.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.5,  0. ]), action=array([-0.99702686], dtype=float32), reward=0.0, last_state=array([ 3. , 23.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.8,  0. ]), action=array([-0.97945845], dtype=float32), reward=0.0, last_state=array([ 4. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.5,  0. ]), action=array([-0.9924793], dtype=float32), reward=0.0, last_state=array([ 8., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.1,  0. ]), action=array([-0.99154896], dtype=float32), reward=0.0, last_state=array([10. , 29.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.4,  0. ]), action=array([-0.9794569], dtype=float32), reward=0.0, last_state=array([13. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 32.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.4,  0. ]), action=array([-0.97188276], dtype=float32), reward=0.0, last_state=array([16. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 32.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 32.1,  0. ]), action=array([-0.9666903], dtype=float32), reward=0.0, last_state=array([18. , 31.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 31.1,  0. ]), action=array([-0.9922168], dtype=float32), reward=0.0, last_state=array([19. , 29.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 29.8,  0. ]), action=array([-0.99600697], dtype=float32), reward=0.0, last_state=array([20. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 28.7,  0. ]), action=array([-0.9487983], dtype=float32), reward=0.0, last_state=array([21. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 26.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 26.6,  0. ]), action=array([-0.999745], dtype=float32), reward=0.0, last_state=array([23. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 25.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 24.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 24.4,  0. ]), action=array([-0.9887528], dtype=float32), reward=0.0, last_state=array([ 1. , 23.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 23.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.1,  0. ]), action=array([-0.9789771], dtype=float32), reward=0.0, last_state=array([ 3. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.8,  0. ]), action=array([-0.98431015], dtype=float32), reward=0.0, last_state=array([ 4. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.3,  0. ]), action=array([-0.9761588], dtype=float32), reward=0.0, last_state=array([ 6. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 24.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 24.7,  0. ]), action=array([-0.99233764], dtype=float32), reward=0.0, last_state=array([11., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([11., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 25.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 25.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 26.8,  0. ]), action=array([-0.96994364], dtype=float32), reward=0.0, last_state=array([14. , 28.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 28.5,  0. ]), action=array([-0.9949334], dtype=float32), reward=0.0, last_state=array([15. , 29.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 29.8,  0. ]), action=array([-0.9948001], dtype=float32), reward=0.0, last_state=array([16. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 30.8,  0. ]), action=array([-0.98879117], dtype=float32), reward=0.0, last_state=array([17. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 31.4,  0. ]), action=array([-0.9877869], dtype=float32), reward=0.0, last_state=array([18. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 31.3,  0. ]), action=array([-0.98594207], dtype=float32), reward=0.0, last_state=array([19. , 30.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 30.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 29.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.3,  0. ]), action=array([-0.99175996], dtype=float32), reward=0.0, last_state=array([22. , 27.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.1,  0. ]), action=array([-0.9900243], dtype=float32), reward=0.0, last_state=array([23. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 25.9,  0. ]), action=array([-0.9727952], dtype=float32), reward=0.0, last_state=array([24. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 24.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 23.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 23.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 22.2,  0. ]), action=array([-0.965049], dtype=float32), reward=0.0, last_state=array([ 3. , 21.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 21.5,  0. ]), action=array([-0.97720015], dtype=float32), reward=0.0, last_state=array([ 4. , 20.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 20.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 20.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.1,  0. ]), action=array([-0.9759116], dtype=float32), reward=0.0, last_state=array([ 6., 20.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 6., 20.,  0.]), action=array([-0.9768482], dtype=float32), reward=0.0, last_state=array([ 7. , 20.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 20.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 21.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 21.7,  0. ]), action=array([-0.9603985], dtype=float32), reward=0.0, last_state=array([ 9. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 23.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 24.8,  0. ]), action=array([-0.9961709], dtype=float32), reward=0.0, last_state=array([11. , 26.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 26.2,  0. ]), action=array([-0.96507174], dtype=float32), reward=0.0, last_state=array([12. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 27.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 28.4,  0. ]), action=array([-0.98781216], dtype=float32), reward=0.0, last_state=array([14., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([14., 29.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 29.3,  0. ]), action=array([-0.99710584], dtype=float32), reward=0.0, last_state=array([16. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 29.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 28.9,  0. ]), action=array([-0.98066837], dtype=float32), reward=0.0, last_state=array([18., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 28.,  0.]), action=array([-0.9682024], dtype=float32), reward=0.0, last_state=array([19. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 26.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 25.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 24.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 23.2,  0. ]), action=array([-0.9944283], dtype=float32), reward=0.0, last_state=array([23., 22.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 22.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 20.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 20.8,  0. ]), action=array([-0.9520134], dtype=float32), reward=0.0, last_state=array([ 1. , 20.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 20.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 20.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 20.4,  0. ]), action=array([-0.99264634], dtype=float32), reward=0.0, last_state=array([ 3. , 20.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 20.3,  0. ]), action=array([-0.98533505], dtype=float32), reward=0.0, last_state=array([ 4. , 20.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 20.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 20.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.1,  0. ]), action=array([-0.9737967], dtype=float32), reward=0.0, last_state=array([ 6., 20.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 6., 20.,  0.]), action=array([-0.99632555], dtype=float32), reward=0.0, last_state=array([ 7., 21.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 7., 21.,  0.]), action=array([-0.97556335], dtype=float32), reward=0.0, last_state=array([ 8. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 9., 25.,  0.]), action=array([-0.9863216], dtype=float32), reward=0.0, last_state=array([10. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 29.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 31.2,  0. ]), action=array([-0.97545004], dtype=float32), reward=0.0, last_state=array([13. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 33.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 34.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 34.2,  0. ]), action=array([-0.99462765], dtype=float32), reward=0.0, last_state=array([16. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 34.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 34.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 34.2,  0. ]), action=array([-0.9867799], dtype=float32), reward=0.0, last_state=array([18. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 31.9,  0. ]), action=array([-0.98188853], dtype=float32), reward=0.0, last_state=array([20. , 30.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.4,  0. ]), action=array([-0.95912266], dtype=float32), reward=0.0, last_state=array([21. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 29.5,  0. ]), action=array([-0.9905738], dtype=float32), reward=0.0, last_state=array([22. , 28.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.6,  0. ]), action=array([-0.9708301], dtype=float32), reward=0.0, last_state=array([23. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.7,  0. ]), action=array([-0.9996743], dtype=float32), reward=0.0, last_state=array([ 1. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.7,  0. ]), action=array([-0.9753868], dtype=float32), reward=0.0, last_state=array([ 4. , 24.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 23.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 23.9,  0. ]), action=array([-0.968367], dtype=float32), reward=0.0, last_state=array([ 7. , 24.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 24.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.2,  0. ]), action=array([-0.9919341], dtype=float32), reward=0.0, last_state=array([ 9. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 25.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 26.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 26.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 25.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 27.5,  0. ]), action=array([-0.99819326], dtype=float32), reward=0.0, last_state=array([13. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 28.4,  0. ]), action=array([-0.96733266], dtype=float32), reward=0.0, last_state=array([14., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([14., 29.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 29.5,  0. ]), action=array([-0.9986381], dtype=float32), reward=0.0, last_state=array([16. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 30.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 30.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 30.6,  0. ]), action=array([-0.9518482], dtype=float32), reward=0.0, last_state=array([18. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 30.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 30.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 29.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.2,  0. ]), action=array([-0.99369466], dtype=float32), reward=0.0, last_state=array([22. , 27.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.1,  0. ]), action=array([-0.99471974], dtype=float32), reward=0.0, last_state=array([23., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 24.9,  0. ]), action=array([-0.9907721], dtype=float32), reward=0.0, last_state=array([ 1., 24.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 1., 24.,  0.]), action=array([-0.9860181], dtype=float32), reward=0.0, last_state=array([ 2., 23.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 23.,  0.]), action=array([-0.9824174], dtype=float32), reward=0.0, last_state=array([ 3. , 22.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.6,  0. ]), action=array([-0.9685203], dtype=float32), reward=0.0, last_state=array([ 4. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 21.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 21.7,  0. ]), action=array([-0.9840546], dtype=float32), reward=0.0, last_state=array([ 6. , 21.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.3,  0. ]), action=array([-0.9743867], dtype=float32), reward=0.0, last_state=array([ 8. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 22.2,  0. ]), action=array([-0.9985067], dtype=float32), reward=0.0, last_state=array([ 9. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 22.3,  0. ]), action=array([-0.99326843], dtype=float32), reward=0.0, last_state=array([10. , 22.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 22.6,  0. ]), action=array([-0.99538136], dtype=float32), reward=0.0, last_state=array([11. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 26.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 26.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 28.2,  0. ]), action=array([-0.98280007], dtype=float32), reward=0.0, last_state=array([14. , 28.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 28.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 29.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 29.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([16., 30.,  0.]), action=array([-0.99392503], dtype=float32), reward=0.0, last_state=array([17. , 29.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 29.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 29.6,  0. ]), action=array([-0.9985881], dtype=float32), reward=0.0, last_state=array([19. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 28.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 27.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 27.8,  0. ]), action=array([-0.96803063], dtype=float32), reward=0.0, last_state=array([21. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 26.8,  0. ]), action=array([-0.99728465], dtype=float32), reward=0.0, last_state=array([22. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 25.7,  0. ]), action=array([-0.99533075], dtype=float32), reward=0.0, last_state=array([23. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 23.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 23.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 22.8,  0. ]), action=array([-0.987504], dtype=float32), reward=0.0, last_state=array([ 2. , 21.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 21.9,  0. ]), action=array([-0.9834565], dtype=float32), reward=0.0, last_state=array([ 3. , 21.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 21.4,  0. ]), action=array([-0.9955107], dtype=float32), reward=0.0, last_state=array([ 4. , 20.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 20.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 20.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 20.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 20.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 21.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 21.3,  0. ]), action=array([-0.9880459], dtype=float32), reward=0.0, last_state=array([ 8. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 25.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 25.3,  0. ]), action=array([-0.99430436], dtype=float32), reward=0.0, last_state=array([10. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 29.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 29.7,  0. ]), action=array([-0.99614036], dtype=float32), reward=0.0, last_state=array([12. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 31.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 32.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([14., 34.,  0.]), action=array([-0.9966906], dtype=float32), reward=0.0, last_state=array([15. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 34.6,  0. ]), action=array([-0.99876064], dtype=float32), reward=0.0, last_state=array([16. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 34.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 34.5,  0. ]), action=array([-0.99909776], dtype=float32), reward=0.0, last_state=array([18. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 33.5,  0. ]), action=array([-0.9791211], dtype=float32), reward=0.0, last_state=array([19., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([19., 32.,  0.]), action=array([-0.98127586], dtype=float32), reward=0.0, last_state=array([20. , 30.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.5,  0. ]), action=array([-0.98612845], dtype=float32), reward=0.0, last_state=array([21., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 29.,  0.]), action=array([-0.98860437], dtype=float32), reward=0.0, last_state=array([22. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 26.1,  0. ]), action=array([-0.98383164], dtype=float32), reward=0.0, last_state=array([24. , 24.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 24.6,  0. ]), action=array([-0.9787667], dtype=float32), reward=0.0, last_state=array([ 1. , 23.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 23.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 22.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 21.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 21.5,  0. ]), action=array([-0.9800132], dtype=float32), reward=0.0, last_state=array([ 4. , 20.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 20.8,  0. ]), action=array([-0.99560773], dtype=float32), reward=0.0, last_state=array([ 5. , 20.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 20.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 20.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7., 21.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 7., 21.,  0.]), action=array([-0.9959901], dtype=float32), reward=0.0, last_state=array([ 8. , 22.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 22.6,  0. ]), action=array([-0.989215], dtype=float32), reward=0.0, last_state=array([ 9. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 26.8,  0. ]), action=array([-0.9897876], dtype=float32), reward=0.0, last_state=array([11. , 28.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 28.8,  0. ]), action=array([-0.98379856], dtype=float32), reward=0.0, last_state=array([12. , 30.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 30.5,  0. ]), action=array([-0.990133], dtype=float32), reward=0.0, last_state=array([13. , 31.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 31.8,  0. ]), action=array([-0.98509943], dtype=float32), reward=0.0, last_state=array([14. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 32.8,  0. ]), action=array([-0.9962081], dtype=float32), reward=0.0, last_state=array([15. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 33.4,  0. ]), action=array([-0.97176963], dtype=float32), reward=0.0, last_state=array([17., 33.,  0.])),\n",
       " ExperienceFirstLast(state=array([17., 33.,  0.]), action=array([-0.9860715], dtype=float32), reward=0.0, last_state=array([18., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 32.,  0.]), action=array([-0.98054963], dtype=float32), reward=0.0, last_state=array([19. , 30.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 30.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 29.,  0.]), action=array([-0.9911537], dtype=float32), reward=0.0, last_state=array([21. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 27.6,  0. ]), action=array([-0.9844363], dtype=float32), reward=0.0, last_state=array([22. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 26.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 24.7,  0. ]), action=array([-0.98959684], dtype=float32), reward=0.0, last_state=array([24. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 23.3,  0. ]), action=array([-0.97747535], dtype=float32), reward=0.0, last_state=array([ 1. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 22.5,  0. ]), action=array([-0.9928636], dtype=float32), reward=0.0, last_state=array([ 2. , 21.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 21.7,  0. ]), action=array([-0.9798236], dtype=float32), reward=0.0, last_state=array([ 3. , 21.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 21.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 20.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 20.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 20.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.4,  0. ]), action=array([-0.9789062], dtype=float32), reward=0.0, last_state=array([ 6. , 20.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 20.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 21.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 21.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 25.6,  0. ]), action=array([-0.977741], dtype=float32), reward=0.0, last_state=array([10., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 28.,  0.]), action=array([-0.9826678], dtype=float32), reward=0.0, last_state=array([11. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 30.3,  0. ]), action=array([-0.9862895], dtype=float32), reward=0.0, last_state=array([12. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([13., 34.,  0.]), action=array([-0.97845864], dtype=float32), reward=0.0, last_state=array([14. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.1,  0. ]), action=array([-0.9889017], dtype=float32), reward=0.0, last_state=array([16. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.3,  0. ]), action=array([-0.97988534], dtype=float32), reward=0.0, last_state=array([17., 36.,  0.])),\n",
       " ExperienceFirstLast(state=array([17., 36.,  0.]), action=array([-0.96148723], dtype=float32), reward=0.0, last_state=array([18. , 34.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.9,  0. ]), action=array([-0.9917468], dtype=float32), reward=0.0, last_state=array([19. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.4,  0. ]), action=array([-0.9984858], dtype=float32), reward=0.0, last_state=array([20. , 31.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.6,  0. ]), action=array([-0.97397625], dtype=float32), reward=0.0, last_state=array([21. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.3,  0. ]), action=array([-0.96830034], dtype=float32), reward=0.0, last_state=array([22. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.9,  0. ]), action=array([-0.9943842], dtype=float32), reward=0.0, last_state=array([23. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.3,  0. ]), action=array([-0.9900917], dtype=float32), reward=0.0, last_state=array([ 1. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.1,  0. ]), action=array([-0.9620498], dtype=float32), reward=0.0, last_state=array([ 3. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 22.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.4,  0. ]), action=array([-0.9970903], dtype=float32), reward=0.0, last_state=array([ 7. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.7,  0. ]), action=array([-0.99577236], dtype=float32), reward=0.0, last_state=array([12. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.1,  0. ]), action=array([-0.9959641], dtype=float32), reward=0.0, last_state=array([14. , 36.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.8,  0. ]), action=array([-0.98681855], dtype=float32), reward=0.0, last_state=array([16. , 36.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.9,  0. ]), action=array([-0.9777711], dtype=float32), reward=0.0, last_state=array([17. , 36.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 36.5,  0. ]), action=array([-0.9998588], dtype=float32), reward=0.0, last_state=array([18. , 35.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 35.5,  0. ]), action=array([-0.9835108], dtype=float32), reward=0.0, last_state=array([19. , 33.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.2,  0. ]), action=array([-0.99585927], dtype=float32), reward=0.0, last_state=array([22. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.2,  0. ]), action=array([-0.97442067], dtype=float32), reward=0.0, last_state=array([23. , 29.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 29.1,  0. ]), action=array([-0.9873432], dtype=float32), reward=0.0, last_state=array([24., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 28.,  0.]), action=array([-0.98160285], dtype=float32), reward=0.0, last_state=array([ 1. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 27.5,  0. ]), action=array([-0.9869236], dtype=float32), reward=0.0, last_state=array([ 2. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 26.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 26.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 26.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 26.1,  0. ]), action=array([-0.9816562], dtype=float32), reward=0.0, last_state=array([ 6. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 26.1,  0. ]), action=array([-0.9921259], dtype=float32), reward=0.0, last_state=array([ 7. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 26.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 28.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 28.6,  0. ]), action=array([-0.98051226], dtype=float32), reward=0.0, last_state=array([ 9. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 29.5,  0. ]), action=array([-0.9808281], dtype=float32), reward=0.0, last_state=array([10., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 31.,  0.]), action=array([-0.9717675], dtype=float32), reward=0.0, last_state=array([11. , 31.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12., 33.,  0.])),\n",
       " ExperienceFirstLast(state=array([12., 33.,  0.]), action=array([-0.98013604], dtype=float32), reward=0.0, last_state=array([13., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([13., 34.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 34.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 34.6,  0. ]), action=array([-0.9650642], dtype=float32), reward=0.0, last_state=array([16. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 33.6,  0. ]), action=array([-0.9995319], dtype=float32), reward=0.0, last_state=array([17. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 32.6,  0. ]), action=array([-0.9917335], dtype=float32), reward=0.0, last_state=array([18. , 31.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 31.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([19., 31.,  0.]), action=array([-0.9746473], dtype=float32), reward=0.0, last_state=array([20. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.3,  0. ]), action=array([-0.98885024], dtype=float32), reward=0.0, last_state=array([21. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 28.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 28.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.3,  0. ]), action=array([-0.9851807], dtype=float32), reward=0.0, last_state=array([ 1. , 26.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.6,  0. ]), action=array([-0.98421913], dtype=float32), reward=0.0, last_state=array([ 2. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.9,  0. ]), action=array([-0.992139], dtype=float32), reward=0.0, last_state=array([ 3. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 25.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.8,  0. ]), action=array([-0.9933789], dtype=float32), reward=0.0, last_state=array([ 5. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 24.5,  0. ]), action=array([-0.9557248], dtype=float32), reward=0.0, last_state=array([ 6. , 24.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.2,  0. ]), action=array([-0.9899207], dtype=float32), reward=0.0, last_state=array([ 7. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 25.2,  0. ]), action=array([-0.98122156], dtype=float32), reward=0.0, last_state=array([ 8. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.7,  0. ]), action=array([-0.97148407], dtype=float32), reward=0.0, last_state=array([ 9. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.4,  0. ]), action=array([-0.9852721], dtype=float32), reward=0.0, last_state=array([10. , 29.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.1,  0. ]), action=array([-0.9990305], dtype=float32), reward=0.0, last_state=array([12. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.8,  0. ]), action=array([-0.981258], dtype=float32), reward=0.0, last_state=array([13. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 33.4,  0. ]), action=array([-0.9666105], dtype=float32), reward=0.0, last_state=array([14. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 33.5,  0. ]), action=array([-0.97179884], dtype=float32), reward=0.0, last_state=array([15. , 33.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 32.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 31.5,  0. ]), action=array([-0.9945241], dtype=float32), reward=0.0, last_state=array([18. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 30.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 29.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 29.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 28.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 27.,  0.]), action=array([-0.9925022], dtype=float32), reward=0.0, last_state=array([24. , 26.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.1,  0. ]), action=array([-0.99855775], dtype=float32), reward=0.0, last_state=array([ 2. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.6,  0. ]), action=array([-0.9879822], dtype=float32), reward=0.0, last_state=array([ 3. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 25.2,  0. ]), action=array([-0.9942315], dtype=float32), reward=0.0, last_state=array([ 4., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 4., 25.,  0.]), action=array([-0.9800304], dtype=float32), reward=0.0, last_state=array([ 5. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 24.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 24.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.9,  0. ]), action=array([-0.9837243], dtype=float32), reward=0.0, last_state=array([ 9. , 28.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.5,  0. ]), action=array([-0.99344003], dtype=float32), reward=0.0, last_state=array([10. , 30.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.4,  0. ]), action=array([-0.96889853], dtype=float32), reward=0.0, last_state=array([11. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.3,  0. ]), action=array([-0.95729154], dtype=float32), reward=0.0, last_state=array([12. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.3,  0. ]), action=array([-0.98409635], dtype=float32), reward=0.0, last_state=array([13. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 33.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([14., 34.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 34.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16., 35.,  0.])),\n",
       " ExperienceFirstLast(state=array([16., 35.,  0.]), action=array([-0.98650396], dtype=float32), reward=0.0, last_state=array([17. , 35.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.4,  0. ]), action=array([-0.99032414], dtype=float32), reward=0.0, last_state=array([18. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 35.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 34.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 34.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.8,  0. ]), action=array([-0.9996646], dtype=float32), reward=0.0, last_state=array([21. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.4,  0. ]), action=array([-0.9750302], dtype=float32), reward=0.0, last_state=array([22. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 28.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.8,  0. ]), action=array([-0.9822937], dtype=float32), reward=0.0, last_state=array([24. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.5,  0. ]), action=array([-0.9836021], dtype=float32), reward=0.0, last_state=array([ 1. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.3,  0. ]), action=array([-0.99034494], dtype=float32), reward=0.0, last_state=array([ 2., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 25.,  0.]), action=array([-0.98298013], dtype=float32), reward=0.0, last_state=array([ 3. , 23.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.7,  0. ]), action=array([-0.99836606], dtype=float32), reward=0.0, last_state=array([ 4., 23.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 4., 23.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.3,  0. ]), action=array([-0.98767936], dtype=float32), reward=0.0, last_state=array([ 6. , 21.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.8,  0. ]), action=array([-0.9963595], dtype=float32), reward=0.0, last_state=array([ 7. , 22.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.4,  0. ]), action=array([-0.9822976], dtype=float32), reward=0.0, last_state=array([ 8. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.8,  0. ]), action=array([-0.9994951], dtype=float32), reward=0.0, last_state=array([ 9. , 27.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.3,  0. ]), action=array([-0.94771487], dtype=float32), reward=0.0, last_state=array([13. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 34.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 35.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.6,  0. ]), action=array([-0.99735665], dtype=float32), reward=0.0, last_state=array([15. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.1,  0. ]), action=array([-0.9962122], dtype=float32), reward=0.0, last_state=array([16. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 35.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.3,  0. ]), action=array([-0.986764], dtype=float32), reward=0.0, last_state=array([19. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.8,  0. ]), action=array([-0.9924373], dtype=float32), reward=0.0, last_state=array([21. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 29.6,  0. ]), action=array([-0.9750774], dtype=float32), reward=0.0, last_state=array([22. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.4,  0. ]), action=array([-0.96301025], dtype=float32), reward=0.0, last_state=array([23. , 27.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.2,  0. ]), action=array([-0.9912233], dtype=float32), reward=0.0, last_state=array([24., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.3,  0. ]), action=array([-0.9575041], dtype=float32), reward=0.0, last_state=array([ 2. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.5,  0. ]), action=array([-0.99067074], dtype=float32), reward=0.0, last_state=array([ 3. , 23.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.5,  0. ]), action=array([-0.98765314], dtype=float32), reward=0.0, last_state=array([ 7. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 25.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.7,  0. ]), action=array([-0.98482275], dtype=float32), reward=0.0, last_state=array([10., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 30.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.1,  0. ]), action=array([-0.98611236], dtype=float32), reward=0.0, last_state=array([13. , 33.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 33.9,  0. ]), action=array([-0.97197545], dtype=float32), reward=0.0, last_state=array([14. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 34.6,  0. ]), action=array([-0.9807862], dtype=float32), reward=0.0, last_state=array([15. , 35.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 35.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 35.3,  0. ]), action=array([-0.987344], dtype=float32), reward=0.0, last_state=array([17. , 34.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 34.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 34.,  0.]), action=array([-0.9742422], dtype=float32), reward=0.0, last_state=array([19. , 32.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.5,  0. ]), action=array([-0.9905072], dtype=float32), reward=0.0, last_state=array([20., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 31.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.6,  0. ]), action=array([-0.99799705], dtype=float32), reward=0.0, last_state=array([23., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 29.,  0.]), action=array([-0.9930319], dtype=float32), reward=0.0, last_state=array([24. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 28.3,  0. ]), action=array([-0.999375], dtype=float32), reward=0.0, last_state=array([ 1., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 1., 28.,  0.]), action=array([-0.9749737], dtype=float32), reward=0.0, last_state=array([ 2. , 27.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 27.8,  0. ]), action=array([-0.9841361], dtype=float32), reward=0.0, last_state=array([ 3. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 27.5,  0. ]), action=array([-0.9996383], dtype=float32), reward=0.0, last_state=array([ 5. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 27.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 27.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 27.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 28.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 29.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 30.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 31.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 32.3,  0. ]), action=array([-0.98192805], dtype=float32), reward=0.0, last_state=array([15. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.4,  0. ]), action=array([-0.9842231], dtype=float32), reward=0.0, last_state=array([16., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([16., 34.,  0.]), action=array([-0.99992263], dtype=float32), reward=0.0, last_state=array([17. , 32.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 32.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 34.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.4,  0. ]), action=array([-0.9640472], dtype=float32), reward=0.0, last_state=array([21. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 32.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 31.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 31.2,  0. ]), action=array([-0.9719309], dtype=float32), reward=0.0, last_state=array([24. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 30.8,  0. ]), action=array([-0.975499], dtype=float32), reward=0.0, last_state=array([ 1. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 30.2,  0. ]), action=array([-0.99273986], dtype=float32), reward=0.0, last_state=array([ 2. , 29.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 29.1,  0. ]), action=array([-0.9955297], dtype=float32), reward=0.0, last_state=array([ 3. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 28.9,  0. ]), action=array([-0.9622061], dtype=float32), reward=0.0, last_state=array([ 4. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 28.9,  0. ]), action=array([-0.99335223], dtype=float32), reward=0.0, last_state=array([ 5. , 28.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 28.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 28.4,  0. ]), action=array([-0.9897571], dtype=float32), reward=0.0, last_state=array([ 7. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 28.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 28.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 30.1,  0. ]), action=array([-0.9910915], dtype=float32), reward=0.0, last_state=array([10., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 31.,  0.]), action=array([-0.9993051], dtype=float32), reward=0.0, last_state=array([11. , 31.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.8,  0. ]), action=array([-0.9903219], dtype=float32), reward=0.0, last_state=array([12. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.3,  0. ]), action=array([-0.98185617], dtype=float32), reward=0.0, last_state=array([13. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 32.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 33.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 33.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 33.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 33.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 33.7,  0. ]), action=array([-0.9450259], dtype=float32), reward=0.0, last_state=array([17. , 33.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 33.8,  0. ]), action=array([-0.9935019], dtype=float32), reward=0.0, last_state=array([18. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 33.4,  0. ]), action=array([-0.97729367], dtype=float32), reward=0.0, last_state=array([19. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 32.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 28.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23., 29.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 29.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 27.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 1., 27.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 3., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.5,  0. ]), action=array([-0.9791935], dtype=float32), reward=0.0, last_state=array([ 5. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 23.9,  0. ]), action=array([-0.98464257], dtype=float32), reward=0.0, last_state=array([ 6. , 23.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 23.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 24.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 24.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.9,  0. ]), action=array([-0.9941187], dtype=float32), reward=0.0, last_state=array([ 9. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 29.5,  0. ]), action=array([-0.99844503], dtype=float32), reward=0.0, last_state=array([10., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 32.,  0.]), action=array([-0.9923464], dtype=float32), reward=0.0, last_state=array([11. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 34.3,  0. ]), action=array([-0.955639], dtype=float32), reward=0.0, last_state=array([12. , 36.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 36.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 37.7,  0. ]), action=array([-0.99499065], dtype=float32), reward=0.0, last_state=array([14. , 38.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 38.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 39.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 39.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 39.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 39.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 38.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 38.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 37.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 37.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 35.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 35.2,  0. ]), action=array([-0.9960305], dtype=float32), reward=0.0, last_state=array([20. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 33.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 32.1,  0. ]), action=array([-0.9710657], dtype=float32), reward=0.0, last_state=array([22. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.7,  0. ]), action=array([-0.9973495], dtype=float32), reward=0.0, last_state=array([23. , 29.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 29.3,  0. ]), action=array([-0.9868832], dtype=float32), reward=0.0, last_state=array([24., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 28.,  0.]), action=array([-0.9909491], dtype=float32), reward=0.0, last_state=array([ 1. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.9,  0. ]), action=array([-0.9950968], dtype=float32), reward=0.0, last_state=array([ 2. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.7,  0. ]), action=array([-0.9583514], dtype=float32), reward=0.0, last_state=array([ 3. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.5,  0. ]), action=array([-0.9760031], dtype=float32), reward=0.0, last_state=array([ 4. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 23.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 23.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.7,  0. ]), action=array([-0.9948977], dtype=float32), reward=0.0, last_state=array([10. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 31.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 33.5,  0. ]), action=array([-0.9831426], dtype=float32), reward=0.0, last_state=array([12. , 35.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 35.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 36.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 37.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 38.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 38.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 38.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 38.3,  0. ]), action=array([-0.9915367], dtype=float32), reward=0.0, last_state=array([17. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.7,  0. ]), action=array([-0.97648174], dtype=float32), reward=0.0, last_state=array([18. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 36.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 34.5,  0. ]), action=array([-0.9843855], dtype=float32), reward=0.0, last_state=array([20. , 32.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.7,  0. ]), action=array([-0.97041965], dtype=float32), reward=0.0, last_state=array([21. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.3,  0. ]), action=array([-0.98341924], dtype=float32), reward=0.0, last_state=array([22. , 29.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.9,  0. ]), action=array([-0.9843251], dtype=float32), reward=0.0, last_state=array([23. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 27.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 1., 26.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 25.,  0.]), action=array([-0.9657909], dtype=float32), reward=0.0, last_state=array([ 3. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.3,  0. ]), action=array([-0.98637295], dtype=float32), reward=0.0, last_state=array([ 5. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.7,  0. ]), action=array([-0.9833222], dtype=float32), reward=0.0, last_state=array([ 6. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.9,  0. ]), action=array([-0.98525393], dtype=float32), reward=0.0, last_state=array([ 8. , 25.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.7,  0. ]), action=array([-0.9955953], dtype=float32), reward=0.0, last_state=array([10., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([10., 30.,  0.]), action=array([-0.9921782], dtype=float32), reward=0.0, last_state=array([11. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.1,  0. ]), action=array([-0.9804389], dtype=float32), reward=0.0, last_state=array([12. , 33.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.3,  0. ]), action=array([-0.9857578], dtype=float32), reward=0.0, last_state=array([14. , 36.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.2,  0. ]), action=array([-0.99324775], dtype=float32), reward=0.0, last_state=array([15. , 36.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 36.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.7,  0. ]), action=array([-0.96634704], dtype=float32), reward=0.0, last_state=array([17. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 36.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.5,  0. ]), action=array([-0.981293], dtype=float32), reward=0.0, last_state=array([21., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 30.,  0.]), action=array([-0.9920926], dtype=float32), reward=0.0, last_state=array([22. , 28.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.5,  0. ]), action=array([-0.9963465], dtype=float32), reward=0.0, last_state=array([23. , 27.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.1,  0. ]), action=array([-0.99957925], dtype=float32), reward=0.0, last_state=array([24. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 25.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 24.5,  0. ]), action=array([-0.97574586], dtype=float32), reward=0.0, last_state=array([ 2. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 21.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 21.5,  0. ]), action=array([-0.9763356], dtype=float32), reward=0.0, last_state=array([ 5. , 20.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 20.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 20.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 20.3,  0. ]), action=array([-0.99598545], dtype=float32), reward=0.0, last_state=array([ 7. , 20.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 20.9,  0. ]), action=array([-0.9740887], dtype=float32), reward=0.0, last_state=array([ 8. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.3,  0. ]), action=array([-0.9808624], dtype=float32), reward=0.0, last_state=array([ 9. , 25.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 25.8,  0. ]), action=array([-0.9920732], dtype=float32), reward=0.0, last_state=array([10. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 28.2,  0. ]), action=array([-0.98811376], dtype=float32), reward=0.0, last_state=array([11. , 30.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 30.3,  0. ]), action=array([-0.9843731], dtype=float32), reward=0.0, last_state=array([12. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 33.6,  0. ]), action=array([-0.9898459], dtype=float32), reward=0.0, last_state=array([14. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 34.5,  0. ]), action=array([-0.99356997], dtype=float32), reward=0.0, last_state=array([15., 35.,  0.])),\n",
       " ExperienceFirstLast(state=array([15., 35.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16., 35.,  0.])),\n",
       " ExperienceFirstLast(state=array([16., 35.,  0.]), action=array([-0.9946639], dtype=float32), reward=0.0, last_state=array([17. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 34.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 33.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 33.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 31.3,  0. ]), action=array([-0.9702053], dtype=float32), reward=0.0, last_state=array([20. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.3,  0. ]), action=array([-0.98877], dtype=float32), reward=0.0, last_state=array([22. , 27.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 25.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 23.9,  0. ]), action=array([-0.9929243], dtype=float32), reward=0.0, last_state=array([ 2. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 21.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 21.9,  0. ]), action=array([-0.95855296], dtype=float32), reward=0.0, last_state=array([ 5. , 21.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 21.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 21.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.1,  0. ]), action=array([-0.98675084], dtype=float32), reward=0.0, last_state=array([ 7. , 21.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 21.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 23.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 28.7,  0. ]), action=array([-0.9838833], dtype=float32), reward=0.0, last_state=array([11. , 30.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 30.9,  0. ]), action=array([-0.9921153], dtype=float32), reward=0.0, last_state=array([12. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13., 34.,  0.])),\n",
       " ExperienceFirstLast(state=array([13., 34.,  0.]), action=array([-0.9416127], dtype=float32), reward=0.0, last_state=array([14. , 34.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 34.7,  0. ]), action=array([-0.99985254], dtype=float32), reward=0.0, last_state=array([15. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 35.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 35.1,  0. ]), action=array([-0.97881705], dtype=float32), reward=0.0, last_state=array([17. , 34.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 34.4,  0. ]), action=array([-0.9709402], dtype=float32), reward=0.0, last_state=array([18. , 33.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 33.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 31.5,  0. ]), action=array([-0.98614115], dtype=float32), reward=0.0, last_state=array([20., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 30.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 28.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.8,  0. ]), action=array([-0.9985399], dtype=float32), reward=0.0, last_state=array([22. , 27.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 26.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 26.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 25.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 25.4,  0. ]), action=array([-0.977706], dtype=float32), reward=0.0, last_state=array([ 1. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 24.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 23.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.6,  0. ]), action=array([-0.9994433], dtype=float32), reward=0.0, last_state=array([ 3. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.2,  0. ]), action=array([-0.9941251], dtype=float32), reward=0.0, last_state=array([ 5. , 21.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 21.6,  0. ]), action=array([-0.98849815], dtype=float32), reward=0.0, last_state=array([ 6. , 21.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 21.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 21.8,  0. ]), action=array([-0.96685016], dtype=float32), reward=0.0, last_state=array([ 8. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.3,  0. ]), action=array([-0.99245745], dtype=float32), reward=0.0, last_state=array([ 9. , 26.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.4,  0. ]), action=array([-0.99727297], dtype=float32), reward=0.0, last_state=array([11. , 31.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.4,  0. ]), action=array([-0.9736114], dtype=float32), reward=0.0, last_state=array([13. , 34.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 34.9,  0. ]), action=array([-0.97793645], dtype=float32), reward=0.0, last_state=array([14. , 35.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 35.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.3,  0. ]), action=array([-0.9974807], dtype=float32), reward=0.0, last_state=array([19. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.4,  0. ]), action=array([-0.9801292], dtype=float32), reward=0.0, last_state=array([20. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21., 30.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 30.,  0.]), action=array([-0.9945308], dtype=float32), reward=0.0, last_state=array([22. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.7,  0. ]), action=array([-0.9750588], dtype=float32), reward=0.0, last_state=array([23. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.5,  0. ]), action=array([-0.9706276], dtype=float32), reward=0.0, last_state=array([ 3. , 23.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.6,  0. ]), action=array([-0.98152274], dtype=float32), reward=0.0, last_state=array([ 4. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.8,  0. ]), action=array([-0.9832789], dtype=float32), reward=0.0, last_state=array([ 8. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.2,  0. ]), action=array([-0.99380606], dtype=float32), reward=0.0, last_state=array([10. , 30.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.9,  0. ]), action=array([-0.9829096], dtype=float32), reward=0.0, last_state=array([11. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 33.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 35.3,  0. ]), action=array([-0.99459785], dtype=float32), reward=0.0, last_state=array([13. , 36.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 37.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.8,  0. ]), action=array([-0.9699778], dtype=float32), reward=0.0, last_state=array([15. , 38.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 38.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 38.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 38.4,  0. ]), action=array([-0.9788181], dtype=float32), reward=0.0, last_state=array([17. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.7,  0. ]), action=array([-0.9987838], dtype=float32), reward=0.0, last_state=array([18. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 36.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 34.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.5,  0. ]), action=array([-0.98621637], dtype=float32), reward=0.0, last_state=array([22. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 30.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.3,  0. ]), action=array([-0.9800013], dtype=float32), reward=0.0, last_state=array([ 1. , 26.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 2., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.2,  0. ]), action=array([-0.9974231], dtype=float32), reward=0.0, last_state=array([ 5. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.5,  0. ]), action=array([-0.9776413], dtype=float32), reward=0.0, last_state=array([ 6. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.5,  0. ]), action=array([-0.99115825], dtype=float32), reward=0.0, last_state=array([ 8. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.1,  0. ]), action=array([-0.9754441], dtype=float32), reward=0.0, last_state=array([ 9. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.6,  0. ]), action=array([-0.9878543], dtype=float32), reward=0.0, last_state=array([10. , 30.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.1,  0. ]), action=array([-0.99776167], dtype=float32), reward=0.0, last_state=array([11. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 34.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 35.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.5,  0. ]), action=array([-0.99498904], dtype=float32), reward=0.0, last_state=array([14. , 36.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 36.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.9,  0. ]), action=array([-0.9669849], dtype=float32), reward=0.0, last_state=array([17. , 36.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 36.2,  0. ]), action=array([-0.9811761], dtype=float32), reward=0.0, last_state=array([18. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.8,  0. ]), action=array([-0.99109316], dtype=float32), reward=0.0, last_state=array([19. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 31.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 30.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 27.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.8,  0. ]), action=array([-0.9542987], dtype=float32), reward=0.0, last_state=array([24. , 26.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.6,  0. ]), action=array([-0.9960638], dtype=float32), reward=0.0, last_state=array([ 1. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.7,  0. ]), action=array([-0.9897006], dtype=float32), reward=0.0, last_state=array([ 2. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3., 24.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 3., 24.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.7,  0. ]), action=array([-0.9778113], dtype=float32), reward=0.0, last_state=array([ 7. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.7,  0. ]), action=array([-0.9930477], dtype=float32), reward=0.0, last_state=array([11., 33.,  0.])),\n",
       " ExperienceFirstLast(state=array([11., 33.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.8,  0. ]), action=array([-0.98320365], dtype=float32), reward=0.0, last_state=array([13. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.3,  0. ]), action=array([-0.9874913], dtype=float32), reward=0.0, last_state=array([14. , 37.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.3,  0. ]), action=array([-0.9997828], dtype=float32), reward=0.0, last_state=array([15. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.7,  0. ]), action=array([-0.9748419], dtype=float32), reward=0.0, last_state=array([16. , 37.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 37.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 35.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.2,  0. ]), action=array([-0.9875055], dtype=float32), reward=0.0, last_state=array([22. , 29.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.9,  0. ]), action=array([-0.99845046], dtype=float32), reward=0.0, last_state=array([23. , 28.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.6,  0. ]), action=array([-0.9746315], dtype=float32), reward=0.0, last_state=array([24. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.2,  0. ]), action=array([-0.9859573], dtype=float32), reward=0.0, last_state=array([ 3. , 24.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.1,  0. ]), action=array([-0.95781606], dtype=float32), reward=0.0, last_state=array([ 4. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.5,  0. ]), action=array([-0.95423216], dtype=float32), reward=0.0, last_state=array([ 5. , 22.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.5,  0. ]), action=array([-0.9755336], dtype=float32), reward=0.0, last_state=array([ 7., 23.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 7., 23.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 25.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 25.5,  0. ]), action=array([-0.99677664], dtype=float32), reward=0.0, last_state=array([ 9. , 28.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.1,  0. ]), action=array([-0.9970296], dtype=float32), reward=0.0, last_state=array([10. , 30.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.6,  0. ]), action=array([-0.99279934], dtype=float32), reward=0.0, last_state=array([11. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.9,  0. ]), action=array([-0.9977526], dtype=float32), reward=0.0, last_state=array([12. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 36.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.2,  0. ]), action=array([-0.96423537], dtype=float32), reward=0.0, last_state=array([14. , 37.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.7,  0. ]), action=array([-0.9804956], dtype=float32), reward=0.0, last_state=array([17., 37.,  0.])),\n",
       " ExperienceFirstLast(state=array([17., 37.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 35.7,  0. ]), action=array([-0.98955077], dtype=float32), reward=0.0, last_state=array([19. , 33.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.7,  0. ]), action=array([-0.97172225], dtype=float32), reward=0.0, last_state=array([20. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.3,  0. ]), action=array([-0.9852907], dtype=float32), reward=0.0, last_state=array([21., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 31.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.7,  0. ]), action=array([-0.9463472], dtype=float32), reward=0.0, last_state=array([ 4. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.5,  0. ]), action=array([-0.99082303], dtype=float32), reward=0.0, last_state=array([ 8. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.6,  0. ]), action=array([-0.9750822], dtype=float32), reward=0.0, last_state=array([11. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 35.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.6,  0. ]), action=array([-0.9751873], dtype=float32), reward=0.0, last_state=array([16. , 36.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.6,  0. ]), action=array([-0.9654183], dtype=float32), reward=0.0, last_state=array([17. , 35.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.8,  0. ]), action=array([-0.9820154], dtype=float32), reward=0.0, last_state=array([20. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.5,  0. ]), action=array([-0.99668324], dtype=float32), reward=0.0, last_state=array([21. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.6,  0. ]), action=array([-0.9977161], dtype=float32), reward=0.0, last_state=array([24. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.3,  0. ]), action=array([-0.99829185], dtype=float32), reward=0.0, last_state=array([ 4. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.2,  0. ]), action=array([-0.99609387], dtype=float32), reward=0.0, last_state=array([ 6. , 21.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.9,  0. ]), action=array([-0.99947387], dtype=float32), reward=0.0, last_state=array([ 9. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13., 36.,  0.])),\n",
       " ExperienceFirstLast(state=array([13., 36.,  0.]), action=array([-0.9995561], dtype=float32), reward=0.0, last_state=array([14. , 37.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 37.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 37.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 36.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 36.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 35.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 35.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.4,  0. ]), action=array([-0.9915044], dtype=float32), reward=0.0, last_state=array([20. , 32.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.2,  0. ]), action=array([-0.99676245], dtype=float32), reward=0.0, last_state=array([21., 31.,  0.])),\n",
       " ExperienceFirstLast(state=array([21., 31.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.9,  0. ]), action=array([-0.9720033], dtype=float32), reward=0.0, last_state=array([23. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 27.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 27.5,  0. ]), action=array([-0.9800028], dtype=float32), reward=0.0, last_state=array([ 1. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.7,  0. ]), action=array([-0.9802857], dtype=float32), reward=0.0, last_state=array([ 2. , 25.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 25.1,  0. ]), action=array([-0.983596], dtype=float32), reward=0.0, last_state=array([ 4. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 24.7,  0. ]), action=array([-0.9951198], dtype=float32), reward=0.0, last_state=array([ 5. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 24.3,  0. ]), action=array([-0.9520339], dtype=float32), reward=0.0, last_state=array([ 6., 24.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 6., 24.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 24.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 24.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 29.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 29.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 31.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 31.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 33.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 35.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 36.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.6,  0. ]), action=array([-0.96689725], dtype=float32), reward=0.0, last_state=array([14. , 37.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.5,  0. ]), action=array([-0.9992617], dtype=float32), reward=0.0, last_state=array([15. , 37.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.9,  0. ]), action=array([-0.98736584], dtype=float32), reward=0.0, last_state=array([16. , 37.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 37.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.2,  0. ]), action=array([-0.9894683], dtype=float32), reward=0.0, last_state=array([18., 36.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 36.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 34.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 32.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 32.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 31.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.7,  0. ]), action=array([-0.9550607], dtype=float32), reward=0.0, last_state=array([23. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 26.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.4,  0. ]), action=array([-0.9856334], dtype=float32), reward=0.0, last_state=array([ 2. , 23.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.9,  0. ]), action=array([-0.9772235], dtype=float32), reward=0.0, last_state=array([ 3. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 22.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 21.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 21.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5., 21.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 5., 21.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 20.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 20.4,  0. ]), action=array([-0.99582624], dtype=float32), reward=0.0, last_state=array([ 7. , 20.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 20.8,  0. ]), action=array([-0.98214614], dtype=float32), reward=0.0, last_state=array([ 8. , 23.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.6,  0. ]), action=array([-0.9885113], dtype=float32), reward=0.0, last_state=array([ 9. , 26.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.4,  0. ]), action=array([-0.9601293], dtype=float32), reward=0.0, last_state=array([10. , 29.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 33.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 33.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13., 35.,  0.])),\n",
       " ExperienceFirstLast(state=array([13., 35.,  0.]), action=array([-0.96890056], dtype=float32), reward=0.0, last_state=array([14. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.1,  0. ]), action=array([-0.99977356], dtype=float32), reward=0.0, last_state=array([15. , 36.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.6,  0. ]), action=array([-0.9951762], dtype=float32), reward=0.0, last_state=array([16. , 36.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.5,  0. ]), action=array([-0.9923579], dtype=float32), reward=0.0, last_state=array([17. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 34.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.2,  0. ]), action=array([-0.9901447], dtype=float32), reward=0.0, last_state=array([19. , 32.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 32.1,  0. ]), action=array([-0.9618682], dtype=float32), reward=0.0, last_state=array([20. , 30.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 29.5,  0. ]), action=array([-0.9583031], dtype=float32), reward=0.0, last_state=array([22. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.3,  0. ]), action=array([-0.9770829], dtype=float32), reward=0.0, last_state=array([23., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 27.,  0.]), action=array([-0.992585], dtype=float32), reward=0.0, last_state=array([24. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 25.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 24.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 24.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.4,  0. ]), action=array([-0.97528964], dtype=float32), reward=0.0, last_state=array([ 6. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 22.1,  0. ]), action=array([-0.99695003], dtype=float32), reward=0.0, last_state=array([ 7. , 22.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 25.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.6,  0. ]), action=array([-0.9973455], dtype=float32), reward=0.0, last_state=array([10. , 30.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 30.2,  0. ]), action=array([-0.985189], dtype=float32), reward=0.0, last_state=array([11. , 32.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.5,  0. ]), action=array([-0.99813], dtype=float32), reward=0.0, last_state=array([12. , 34.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.1,  0. ]), action=array([-0.9696298], dtype=float32), reward=0.0, last_state=array([13. , 35.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 36.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 36.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.3,  0. ]), action=array([-0.99450773], dtype=float32), reward=0.0, last_state=array([16., 36.,  0.])),\n",
       " ExperienceFirstLast(state=array([16., 36.,  0.]), action=array([-0.9568269], dtype=float32), reward=0.0, last_state=array([17. , 35.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 35.1,  0. ]), action=array([-0.9916691], dtype=float32), reward=0.0, last_state=array([18. , 33.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 33.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 31.9,  0. ]), action=array([-0.9753498], dtype=float32), reward=0.0, last_state=array([20. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 30.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 29.6,  0. ]), action=array([-0.9693663], dtype=float32), reward=0.0, last_state=array([22. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 28.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 27.3,  0. ]), action=array([-0.99545425], dtype=float32), reward=0.0, last_state=array([24. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.2,  0. ]), action=array([-0.98108757], dtype=float32), reward=0.0, last_state=array([ 2. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 22.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 21.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.3,  0. ]), action=array([-0.9937416], dtype=float32), reward=0.0, last_state=array([ 8. , 24.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 27.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([10. , 29.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 32.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 32.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([12. , 34.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 34.2,  0. ]), action=array([-0.9743924], dtype=float32), reward=0.0, last_state=array([13. , 35.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 35.7,  0. ]), action=array([-0.9794247], dtype=float32), reward=0.0, last_state=array([14. , 36.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 36.7,  0. ]), action=array([-0.9886626], dtype=float32), reward=0.0, last_state=array([15. , 37.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 37.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 37.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 37.2,  0. ]), action=array([-0.99577016], dtype=float32), reward=0.0, last_state=array([17. , 36.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 36.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 35.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 35.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 33.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.1,  0. ]), action=array([-0.9911693], dtype=float32), reward=0.0, last_state=array([20. , 31.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 31.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 28.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 28.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([24., 27.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 26.1,  0. ]), action=array([-0.9958352], dtype=float32), reward=0.0, last_state=array([ 2. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 25.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 24.3,  0. ]), action=array([-0.9823489], dtype=float32), reward=0.0, last_state=array([ 4. , 23.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 23.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 23.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 23.4,  0. ]), action=array([-0.97117865], dtype=float32), reward=0.0, last_state=array([ 6. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 23.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 23.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 23.5,  0. ]), action=array([-0.9852561], dtype=float32), reward=0.0, last_state=array([ 8., 26.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 8., 26.,  0.]), action=array([-0.9904083], dtype=float32), reward=0.0, last_state=array([ 9. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.7,  0. ]), action=array([-0.9839808], dtype=float32), reward=0.0, last_state=array([10. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 31.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 33.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 33.4,  0. ]), action=array([-0.9717439], dtype=float32), reward=0.0, last_state=array([12. , 35.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 35.3,  0. ]), action=array([-0.9807029], dtype=float32), reward=0.0, last_state=array([13. , 36.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 36.7,  0. ]), action=array([-0.9957534], dtype=float32), reward=0.0, last_state=array([14. , 37.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 37.8,  0. ]), action=array([-0.9984797], dtype=float32), reward=0.0, last_state=array([15. , 38.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 38.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 38.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 38.3,  0. ]), action=array([-0.99652433], dtype=float32), reward=0.0, last_state=array([17. , 37.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 37.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 36.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 36.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([19. , 34.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 34.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([20. , 33.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 33.6,  0. ]), action=array([-0.97648835], dtype=float32), reward=0.0, last_state=array([21. , 32.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 32.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 31.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 31.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 30.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 30.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([24. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 29.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 28.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 28.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 27.4,  0. ]), action=array([-0.9869149], dtype=float32), reward=0.0, last_state=array([ 3. , 26.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 26.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 4. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 25.7,  0. ]), action=array([-0.98323745], dtype=float32), reward=0.0, last_state=array([ 5. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 25.1,  0. ]), action=array([-0.99003136], dtype=float32), reward=0.0, last_state=array([ 6. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7., 25.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 7., 25.,  0.]), action=array([-0.97432506], dtype=float32), reward=0.0, last_state=array([ 8. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 26.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 28.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 28.3,  0. ]), action=array([-0.9897829], dtype=float32), reward=0.0, last_state=array([10. , 29.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 29.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.3,  0. ]), action=array([-0.9785488], dtype=float32), reward=0.0, last_state=array([12. , 32.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.9,  0. ]), action=array([-0.9911543], dtype=float32), reward=0.0, last_state=array([13. , 34.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 34.7,  0. ]), action=array([-0.97799534], dtype=float32), reward=0.0, last_state=array([14. , 35.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 35.8,  0. ]), action=array([-0.98495775], dtype=float32), reward=0.0, last_state=array([15. , 36.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 36.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 36.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 36.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17., 36.,  0.])),\n",
       " ExperienceFirstLast(state=array([17., 36.,  0.]), action=array([-0.9804331], dtype=float32), reward=0.0, last_state=array([18. , 34.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 34.8,  0. ]), action=array([-0.9994226], dtype=float32), reward=0.0, last_state=array([19. , 33.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 33.3,  0. ]), action=array([-0.9925516], dtype=float32), reward=0.0, last_state=array([20., 32.,  0.])),\n",
       " ExperienceFirstLast(state=array([20., 32.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 30.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 30.7,  0. ]), action=array([-0.99882066], dtype=float32), reward=0.0, last_state=array([22. , 29.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 29.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23., 28.,  0.])),\n",
       " ExperienceFirstLast(state=array([23., 28.,  0.]), action=array([-0.9859838], dtype=float32), reward=0.0, last_state=array([24. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 26.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 1. , 25.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 25.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 24.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 24.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 3. , 23.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.3,  0. ]), action=array([-0.9808134], dtype=float32), reward=0.0, last_state=array([ 4. , 22.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 22.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 5. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6. , 21.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 6. , 21.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7., 22.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 7., 22.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 24.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 24.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 26.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 26.7,  0. ]), action=array([-0.98137575], dtype=float32), reward=0.0, last_state=array([10. , 28.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 28.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 31.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 31.2,  0. ]), action=array([-0.9951313], dtype=float32), reward=0.0, last_state=array([12. , 32.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 32.8,  0. ]), action=array([-0.99391776], dtype=float32), reward=0.0, last_state=array([13. , 33.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 33.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 34.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 34.3,  0. ]), action=array([-0.97152615], dtype=float32), reward=0.0, last_state=array([15. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 34.5,  0. ]), action=array([-0.9950991], dtype=float32), reward=0.0, last_state=array([16. , 34.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 34.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 33.8,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 33.8,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18. , 32.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([18. , 32.4,  0. ]), action=array([-0.98365796], dtype=float32), reward=0.0, last_state=array([19. , 30.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 30.6,  0. ]), action=array([-0.987794], dtype=float32), reward=0.0, last_state=array([20. , 29.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 29.5,  0. ]), action=array([-0.9825484], dtype=float32), reward=0.0, last_state=array([21. , 28.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 28.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([22. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 26.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([23. , 26.2,  0. ]), action=array([-0.9957757], dtype=float32), reward=0.0, last_state=array([24. , 25.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([24. , 25.1,  0. ]), action=array([-0.9975969], dtype=float32), reward=0.0, last_state=array([ 1. , 24.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 1. , 24.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 2. , 23.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 2. , 23.7,  0. ]), action=array([-0.9861364], dtype=float32), reward=0.0, last_state=array([ 3. , 23.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 3. , 23.1,  0. ]), action=array([-0.99383277], dtype=float32), reward=0.0, last_state=array([ 4. , 21.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 4. , 21.6,  0. ]), action=array([-0.9925072], dtype=float32), reward=0.0, last_state=array([ 5. , 22.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 5. , 22.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 6., 22.,  0.])),\n",
       " ExperienceFirstLast(state=array([ 6., 22.,  0.]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 7. , 22.5,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 7. , 22.5,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 8. , 23.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 8. , 23.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([ 9. , 25.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([ 9. , 25.3,  0. ]), action=array([-0.97718793], dtype=float32), reward=0.0, last_state=array([10. , 26.1,  0. ])),\n",
       " ExperienceFirstLast(state=array([10. , 26.1,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([11. , 27.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([11. , 27.9,  0. ]), action=array([-0.99187565], dtype=float32), reward=0.0, last_state=array([12. , 28.9,  0. ])),\n",
       " ExperienceFirstLast(state=array([12. , 28.9,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([13. , 29.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([13. , 29.2,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([14. , 28.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([14. , 28.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([15. , 27.6,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 27.6,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([16. , 27.3,  0. ])),\n",
       " ExperienceFirstLast(state=array([16. , 27.3,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([17. , 27.4,  0. ])),\n",
       " ExperienceFirstLast(state=array([17. , 27.4,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([18., 27.,  0.])),\n",
       " ExperienceFirstLast(state=array([18., 27.,  0.]), action=array([-0.98670983], dtype=float32), reward=0.0, last_state=array([19. , 26.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([19. , 26.2,  0. ]), action=array([-0.9675371], dtype=float32), reward=0.0, last_state=array([20. , 25.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([20. , 25.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([21. , 25.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([21. , 25.2,  0. ]), action=array([-0.98650503], dtype=float32), reward=0.0, last_state=array([22. , 24.7,  0. ])),\n",
       " ExperienceFirstLast(state=array([22. , 24.7,  0. ]), action=array([-1.], dtype=float32), reward=0.0, last_state=array([23. , 24.2,  0. ])),\n",
       " ExperienceFirstLast(state=array([15. , 12.9,  1. ]), action=array([0.6208673], dtype=float32), reward=-3.904124750533754, last_state=array([16. , 13.2,  1. ])),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer[4].buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(buffer[4].buffer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cost[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cost[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-7bc03a594d97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtotal_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtotal_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "total_cost=[]\n",
    "for i in range(len(cost[4])):\n",
    "    print(i)\n",
    "    if cost[4][i]==0.0:\n",
    "        total_cost.append(0.0)\n",
    "    else:\n",
    "        total_cost.append(cost[4][i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buildings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5430cfbc29c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0muid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuildings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mrew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_electric_consumption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfilename_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CityLearn_DDPG.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'buildings' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rew = {}\n",
    "for uid in buildings:\n",
    "    rew[uid] = env.total_electric_consumption\n",
    "filename_results = 'CityLearn_DDPG.csv'\n",
    "results = pd.DataFrame.from_dict(rew)\n",
    "results.to_csv(filename_results,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('CityLearn_DDPG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(data[\"4\"][0].split('[')[1].split(']')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"4\"][8][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"4\"][8][0]=='['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['4'].apply(lambda x: x.split('[')[1].split(']')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = []\n",
    "for i in range(len(data['4'])):\n",
    "    if data[\"4\"][i][0]=='[':\n",
    "        demand.append(float(data[\"4\"][i].split('[')[1].split(']')[0]))\n",
    "    else:\n",
    "        demand.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO2de5gV1ZXof0tUNE6ML5IxYIIxzEMzN2qIccbcScZkEDUzOHNjRmduJBnvkDj6JZnk3gmaTDQqN+alkUSNJhLRa0RjMKCgiIiKiEDzpkGggQaaVzc0j+bR3XT3un+c3d3V3VXnVNWpc06dc9bv+/rrOqt27dq7atdee6+99t6iqhiGYRjVzXGlToBhGIZRekwZGIZhGKYMDMMwDFMGhmEYBqYMDMMwDOD4UicgLmeddZYOHz681MkwDMMoK5YsWbJHVYf0l5etMhg+fDg1NTWlToZhGEZZISJb/ORmJjIMwzBMGRiGYRimDAzDMAxMGRiGYRiYMjAMwzAwZWAYhmFgysAwDMPAlEEoVJW3Nu6hsaWVA0eOcbS9k5drd3G4rQOAN9Y3sX3/0Z7wdY0tHGw9RmeX8vr6JgC6upSmljbW725hxbb9zF6zm3W7WgDYf6SdGSt30nqsk64u5VhnFztcfF1dyqLNzTz4Wh0dnV2Badx/pJ1ZtbsCz3d25V6qfNry7SzZsi/3A0mIhn1HeKtuT0Hv0dmlTFu+na4Q+Q/iSHsHb7j3uOtAK8u2Fu8ZGUaxCD3pTEQGATXAdlX9nIicC0wBzgCWAl9U1XYRGQw8DnwM2Av8k6rWuzhuBW4EOoGvqeosJx8N3A8MAn6tqvcklL9EeHLhVr77h9UD5H/30ffz8+sv4oZJizjlxEHU3jkagM/e+wbnn30qh9s72LL3CN//+wv4w/LtLNu6f0Ac9fdczc2/Xcr8ur0AfOmvhtPW0cVTi7ay4vZRPPrmZibO2QDAIBG+8qnzfNP4lSeWsHBzM4u/81mGvHtwn3Nz32nky48tZsbXPskF73+P7/Utrcf4+pTlPWkqBpf/5HXaO7sKer9Jb25mwsy1tHd0ce3Ic2LFcevUVUxbvoNXv/Uprrx/Hm0dhU2zYZSCKD2DrwNrPb9/CNynqiOAfWQqedz/far6YeA+Fw4ROR+4DrgAGA08KCKDnJJ5ALgSOB+43oVNDQs3N/vKtzYf6Tk+3N7Z59yanQfZsjdzft6GJl9F0M32fb29iqcXb+O1dY2ZONs6ePWd3T3n9h5uD4yjOy3HfHoPr6zNxLE0SxraO4J7HYWiPUtPJymaDrUB0Jzl2eViY9MhAA63ddJWgudkGMUglDIQkWHA1cCv3W8BLgeedUEmA9e44zHuN+78Z1z4McAUVW1T1c1AHXCJ+6tT1U2q2k6mtzEm34wZhmEY4QnbM/gZ8J9Ad7PoTGC/qna43w3AUHc8FNgG4M4fcOF75P2uCZIPQETGiUiNiNQ0NTWFTLphGIaRi5zKQEQ+BzSq6hKv2Ceo5jgXVT5QqPqIqo5U1ZFDhgxYdM8wDMOISZgB5MuAvxeRq4CTgFPJ9BROE5HjXet/GLDDhW8AzgEaROR44D1As0fejfeaILlhGEbq6OxSBh3n144tX3L2DFT1VlUdpqrDyQwAv6qq/wLMBT7vgo0Fprnj6e437vyrqqpOfp2IDHaeSCOARcBiYISInCsiJ7p7TE8kd4ZhGAmzcNNezrttJjX1/o4l5Uo+8wy+DXxTROrIjAk86uSPAmc6+TeB8QCqWgs8A6wBXgJuVtVO17O4BZhFxlvpGRfWMAwjdbzp5sa8tXFviVOSLJE2t1HV14DX3PEmMp5A/cO0AtcGXD8BmOAjnwnMjJKWVKDhJjKFDBYinjwjSiohZUgSOddEYjHKiQNHjzH4+OM46YRBpU5KwbEZyCHI1zKYTxUiIe+erZ6XEFFImEBlSBK5CvsOjMrjo99/mb//xZulTkZRMGWQApKsiCu0TjeMkrF+96FSJ6EomDJIIVVszTEMo0SYMkgx1so3DKNYmDIwDMMwTBkYhmHEodLMuaYMDMMwIlCp1ltTBnkQtmEQZX5AtnGCSmuJFJMknp09f6OSMWUQgmIO5PavcJK8d7a6rFJbO0lkzAbyjWrAlEEVYJOmDMPIhSmDIlAM64ItlWAYxWfBxr3808MLsu5PXi5EWpvIKAxJttutF2AYxeObzyxn54FWGlvaeP9pJ5c6OXlhPQPDMAzDlIFhGEYYVm8/QOuxzp7flWaaNTNRyjDPFcNIH3sOtfG5n7/JmAvfzwfPPKXUySkI1jPIg2L7nVdWO6S4VForziguh9s6AFi2dX+JU1I4cioDETlJRBaJyAoRqRWR7zv5YyKyWUSWu78LnVxEZKKI1InIShG52BPXWBHZ4P7GeuQfE5FV7pqJUmGL60dRGoVUMNU4aSrJAfUqfHxGFRHGTNQGXK6qh0TkBOBNEXnRnfs/qvpsv/BXktnfeATwCeAh4BMicgZwOzCSzHe1RESmq+o+F2Yc8DaZHc9GAy+SEkqpmRLZnCXU5jYJ3KiMONLeQduxLk4/5cScYavs0RhVSs6egWbo3t3hBPeXrZE0BnjcXfc2cJqInA1cAcxW1WanAGYDo925U1V1gWbWbXgcuCaPPBlGTj7709e56K7ZpU6GUSFUQq8x1JiBiAwSkeVAI5kKfaE7NcGZgu4TkcFONhTY5rm8wcmyyRt85H7pGCciNSJS09TUFCbpqaAok84qoTQWkR0HWkudBKMCqKReYyhloKqdqnohMAy4REQ+AtwK/BnwceAM4NsuuN/z0Rhyv3Q8oqojVXXkkCFDwiS9PEiwRFWbuccwjGSI5E2kqvuB14DRqrrTmYLagN8Al7hgDcA5nsuGATtyyIf5yKsSq8wNI714vdIqrTcexptoiIic5o5PBj4LvONs/TjPn2uA1e6S6cANzqvoUuCAqu4EZgGjROR0ETkdGAXMcudaRORSF9cNwLRks1m+VFh5M4yyxOuVVqnttTDeRGcDk0VkEBnl8YyqviAir4rIEDLPZjnwVRd+JnAVUAccAb4MoKrNInIXsNiFu1NVm93xTcBjwMlkvIgK7km0+2Arp550Au0dXbznXSf0yPcdbuddgwehCq3HOjntXbm9TQrJyoYDJb2/YRjVQU5loKorgYt85JcHhFfg5oBzk4BJPvIa4CO50pIU63a1cMXP3uj5XX/P1T3HF901m09++Cz2Hm5n7c6Dfc71J7GJTJ5oVIPjDdMtzRYmyiY7lUYym9tU7/MzKp+qXI5i857DWc+/Wbenz+9SzYGLMmEqWxLDxFKpq50m8upsIMfIQSU0FGw5CsMwjJhU0mIJpgwMwzAS4JtPL2fk3cETGVWVKYu20tbRGRimlJgyqBAqoJdqGKng5dpdPFOzzfdctu9s6rLt7DnUHnh+xqqdjJ+6ip+9siHfJBaEqhwzKDY57Ymenma+vc4K6rUaRkkY98QSAL4wsndalN93FbX91dKaWfl03+FghVFKrGdgGIYRgUptcJkyMAzDMKpTGSSl2Qthp886TyDPeQ02rJAf9vyMqKgq3352JYvrm3MHLjFVqQyiUu69wlDub+WeyQASmWaQQBxGZRPUiOvoUp6u2cb1j7xd3ATFwJSBkWr2HW6viAk9hpF2TBkYqaV+z2Euums2v5lfX+qkGEZipLVtY8ogZVSqp0IctjQfAWDuusai37ujs4sH5tZxtD2dE4SM0pBPRZ72T9vmGRiGD88uaeDHs9b1+IYbxgDS2sSPifUMikAxykxlFcvS03os0yM42m7KwOhLpS7qWJXKIG2vMsn0ZIurwhoyhpEXqso7uw6WOhmpIcxOZyeJyCIRWSEitSLyfSc/V0QWisgGEXlaRE508sHud507P9wT161Ovk5ErvDIRztZnYiMTz6b8Rk+fgYrt/tvMFO74yCH2nK3HPsviZ2NI+2d7D7Y5nvOW5m/uGonX5+yLGtcP5m1jolzNrB8234A7nxhDcPHz+CIp7U7dWkDw8fP4OqJ83pk35u2uk88jy+o58O3zURVmV+3h7+993X++kdzqd3h/1yeWrSV55Y1+J67depKvv3syqzpLgRJeCSZMq0sfjO/ntE/m8eizYWfA5Ct6PzgxbV8fMIrAPyvyYtzfteFIkzPoA24XFU/ClwIjHbbWf4QuE9VRwD7gBtd+BuBfar6YeA+Fw4ROR+4DrgAGA08KCKD3A5qDwBXAucD17uwqaGu8VDguV0HjhYxJb3c9ORSpi3PvlX0L+bWce/s9WzZ23f/hqaWXmXz2Fv1ADTs683H4wu29Al/x/RaOrqULoVvPrOcDY2H2Np8hPsDFty6deoq/uPpFb7nnlq0jacDFgErBEkMyNugfmWy2jVmtjpHhTjkKhthis7Dr2/q+SZfWduY87suFDmVgdv0vrs2PMH9KXA58KyTTyazDzLAGPcbd/4zbm/jMcAUVW1T1c1ktsW8xP3VqeomVW0HpriwRhWiqvzopXfYuvdI6sx5hlHJhBozcC345UAjMBvYCOxX1W57QwMw1B0PBbYBuPMHgDO98n7XBMn90jFORGpEpKapqSlM0otOOUw7TzMbmw7z4GsbGfdETamTAtjAvJE8iW2XmzChlIGqdqrqhcAwMi35P/cL5v77Neg0htwvHY+o6khVHTlkyJDcCS8Ba3fagFR+ZF79sc6ukqaiknawMvKnGsyNkbyJVHU/8BpwKXCaiHTPUxgGdBu6GoBzANz59wDNXnm/a4LkhmEYqSWd7fv4hPEmGiIip7njk4HPAmuBucDnXbCxwDR3PN39xp1/VTOuHNOB65y30bnACGARsBgY4byTTiQzyDw9icxlyVNicRXTw+Sxt+rZdaC1eDfsh60RZBjpb+HHJcwM5LOByc7r5zjgGVV9QUTWAFNE5G5gGfCoC/8o8ISI1JHpEVwHoKq1IvIMsAboAG5W1U4AEbkFmAUMAiapam1iOSxj/Ard3HWNXH/JBwbIs9XT+So/EfG9gakGoxIohttxOTSkcioDVV0JXOQj30Rm/KC/vBW4NiCuCcAEH/lMYGaI9FYkidqnI0YVtYyWYvZlEt9RMt9i+j9oIzxJlOWcrqVl1I2oyhnIRvpREhq0y/ODV03fjHXDKASmDBImjRVHGtMUTDpSW0YNOqOIlIO5Jy6mDAzDqHqaD7ezcNPewPPl6nQShapUBuXc6CtlQUppGTaMvPnnX73NPxV4a8q0r3ZalcogScqhgsy3URN0eakUk6rSUeJJaUZl8c6ulsjXpLWFHxdTBkbq6T99/7bnVvPh77xYotQYlUbUOj3d7fv4mDKoMLbsjb8CYxiKNrCqwd3qpxZtLVIijEomybKca70hDRGm1JgySJoSu6Fc+8sFJb1/vqTNiyftH7BRWuKMA6StjHdjyqBC6epKvhKrNBtpNlL6vRpGwTBlUGaEbaku3bovsXumtSVjGEmTax5B1PaQX3xpbVRVpTJI1FaY0hdbgI6BDynNvGEkTL5VhmCupUYqSHchNAyj9JgyMFKJt8+R1t6XUSFY+QJMGSSOtcHzw/v80jBWYYqockmyeOVcwjrBexWKMPsZGGXGD15cy8JNye/FrGj1KLs0aCIj9VRSMQmz09k5IjJXRNaKSK2IfN3J7xCR7SKy3P1d5bnmVhGpE5F1InKFRz7ayepEZLxHfq6ILBSRDSLytNvxzPAhTEv14dc3sXzb/sTumfaBL8Mw8idMz6AD+JaqLhWRdwNLRGS2O3efqv7EG1hEziezu9kFwPuBV0TkT9zpB4C/JbPv8WIRma6qa4AfurimiMgvgRuBh/LNnB9/+t0XaetIbl2bcpiUlK31EjX95WrLTyKp5ZRfIzd+rzP3jmUFSUoqyNkzUNWdqrrUHbeQ2f94aJZLxgBTVLVNVTcDdWR2RLsEqFPVTaraDkwBxkhmbdjLgWfd9ZOBa+JmKBdJKoJ0EVxKq7ldn8gGOZVkCzAGEuL1Jrt0RTqJNIAsIsPJbIG50IluEZGVIjJJRE53sqHANs9lDU4WJD8T2K+qHf3kfvcfJyI1IlLT1NQUJempxqqagXgn61Rya8yoIjwf+qzaXQwfP4OW1mOlS08/QisDEfkj4PfAN1T1IBkzznnAhcBO4KfdQX0u1xjygULVR1R1pKqOHDJkSNik52RVwwF27D+aSFzWiMwPbys8DY/S9FAVUIKX/PNXNwBQv6ewC0tGIZQ3kYicQEYRPKmqUwFUdbfn/K+AF9zPBuAcz+XDgB3u2E++BzhNRI53vQNv+KLwd794E4D6e64u5m1zkobK0Es1tdDT9uyN5Mn3HStaUd9EGG8iAR4F1qrqvR752Z5g/wCsdsfTgetEZLCInAuMABYBi4ERznPoRDKDzNM1Yw+YC3zeXT8WmJZftiqXkpQ98T00jKokTu+/HJRGmJ7BZcAXgVUistzJbgOuF5ELydRP9cBXAFS1VkSeAdaQ8US6WVU7AUTkFmAWMAiYpKq1Lr5vA1NE5G5gGRnlUxaUw0suFFWcdaNCibzRTQW1jnIqA1V9E/8G4cws10wAJvjIZ/pdp6qbyHgbGQUg3wLbXrEeWIaRQaQ0Dbs0uabbchRVSFtHF1fdP49Fm5OfpZwUfeYzJPDBJPGhp+ezNcqdNE7kNGWQMGl8yf3TtKnpEGt2HuR701azevvBEqXKHwn8kUB8JYzDSDfF7BVEvVdXl3Lvy+vYe6itMAlymDKoEKp57KKQ2HOtfIplqonbqJi/cQ8TX63j1qmrEk1Pf0wZpIBINn2rnYpCJQ0MGv6UyzvucDtVFXr1BFMGeVJtdbN3UliuLQINo5LxFv+4X0KaPiFTBh7mbaicJS68BLWA9h1pL25CDCPlhGngJDFLPo29ElMGHr746KJSJ6Go7D5Y2AGpfEhTi8mofMJWzqV0BS30nU0ZJEwaNX5SFKOC9nt+phiMUlNKL8Fi3dmUQYLsPtjK/iPpWYUwmPLQWEl+gPFbdLZ6qlEd2LaXCfKJ/zun4Pew+qgXVc2910CIrtr63S2cMvh4hp52cu9lHkVUyb09o7RKPk3fsymDPEnTdPIgrC7Lzqj73gDSt2qtUViqwfQTBTMTVQGF2qkr/WrQMKKRdJlOMr5Cu3KbMqgQKq1iLocel1E5xGku9d0PvHDltVjbrpoySAFJdFeLMQGsGBV0Gtd2MowgKml/bFMGhmEYIUm6zZWmWfxhdjo7R0TmishaEakVka87+RkiMltENrj/pzu5iMhEEakTkZUicrEnrrEu/AYRGeuRf0xEVrlrJkolqduESVHZCU3z4XZG3fc6m/ccTjTecnwWRnmSeI2UwiouTM+gA/iWqv45cClws4icD4wH5qjqCGCO+w1wJZmtLkcA44CHIKM8gNuBT5DZyOb2bgXiwozzXDc6/6wZQRS7HM6q3cX63Yd4+PWNxb2xYaSQtI6H5VQGqrpTVZe64xZgLTAUGANMdsEmA9e44zHA45rhbTKb3Z8NXAHMVtVmVd0HzAZGu3OnquoCtx/y4564qoJiF45qbFEnsrlNNT44IzHiGjxSOQNZRIYDFwELgfep6k7IKAzgvS7YUGCb57IGJ8smb/CRVw3rdx/yPxGhFBTDslaOg7vJbG5Tfvk24lHN+j60MhCRPwJ+D3xDVbNtj+X35WgMuV8axolIjYjUNDVV5gqjaSZMD6aj0/ZLNsqPfBtTlaBDQikDETmBjCJ4UlWnOvFuZ+LB/W908gbgHM/lw4AdOeTDfOQDUNVHVHWkqo4cMmRImKRXDWkxYXz4Oy8GnoubxKDL0pFjo1pRzb/nmaYyHMabSIBHgbWqeq/n1HSg2yNoLDDNI7/BeRVdChxwZqRZwCgROd0NHI8CZrlzLSJyqbvXDZ64jJBEKVRhGkFJKpdyXvM9JTrWKDCdXeFetJJMuYw1ya3AZTHM2kSXAV8EVonIcie7DbgHeEZEbgS2Ate6czOBq4A64AjwZQBVbRaRu4DFLtydqtrsjm8CHgNOBl50f1XPV59YUuokVC1pUERG8dnQ2OIrL2VxKFZZzKkMVPVNgp/FZ3zCK3BzQFyTgEk+8hrgI7nSkkYKqa2Xbt2fM8zq7Qf4x4feor3A+6P6Ya1mo9I42t5Z6iSUDJuBXOZMmr85pyJIqmWRb+UfxYXWFI1RLDTguCg3TBGmDIxIxFEsUa4pVJc4ie8vpd+wEZNimwJ93SYjFKpCz0cyZVAFHIvh7pmqlnlAWsJtXp7A/W38oGqI7fEW8bpIDaQiFUBTBmVGHC+f/hvff6VMBqbTUAenSimmgNZjnQwfP8OWFukmDYU0IUwZlDnlNDu2nCrW8nmqxaV7j+9J8zeXOCWlwVuG07rGUFxMGRixCfsplJPCMgxfxHtYmeXZlEGZUcq2SDHvXU69CKNyKP5M/vR42JkyKDMeeq04ttpyqIvLIY2VSiUp676mnyLcj2hmyGJ5PZkyKDMaW/oOBhfdPS6P+8X90NJom+0KuXxBpVFJM7PL6dspBqYMjMIT8yNIdFnumE1ZryLyRnGovSPfFBkppJJ6PFExZWBUNHEH+6TPgKFh5KL8tYgpgzyptpbE4bbetVvSsmy2UXyq9833zXmUhkLey7nYALKRjUK1Wv0qelWl+XB7HnHmk6Li8PslDew8cDRnuHLISyGo1l6Sbw8zZhmI+gxTue2lYcShUJ4TSVfIR9o7+NbvVnD9I28nG7FRNoRxVihlGS0kpgwMw9HtINTUz2PLGEg5VXJBdG9oc/SYZ9nqCshXXEwZGKknlRVPGtNUDCrITvSHZZndde9/ZX1R75vWohNm28tJItIoIqs9sjtEZLuILHd/V3nO3SoidSKyTkSu8MhHO1mdiIz3yM8VkYUiskFEnhaRE5PMYKWTdt9lL1HnC5Qqb33Wt0/rl2vkTbtbzfdwkTa08SvPaSpfYXoGjwGjfeT3qeqF7m8mgIicD1wHXOCueVBEBonIIOAB4ErgfOB6Fxbghy6uEcA+4MZ8MlRs0jghKgn8chU3p4nOF4hJmLRLn+OBaa7Ud230EntiZOQlrEv/TfQnpzJQ1TeA5lzhHGOAKarapqqbyeyDfIn7q1PVTaraDkwBxkjmiVwOPOuunwxcEzEPRgWSlNtqIpuXp++7TQGmGCGdlXpc8hkzuEVEVjoz0ulONhTY5gnT4GRB8jOB/ara0U/ui4iME5EaEalpamrKI+lGWonycRWqpR5GD1VrL6FSV+wMS5rMOkkTVxk8BJwHXAjsBH7q5L47u8WQ+6Kqj6jqSFUdOWTIkGgprlCq/eNMkgpq5BkxCarsfe39hU2Kz/0Ke8fj41ykqru7j0XkV8AL7mcDcI4n6DBghzv2k+8BThOR413vwBveqDTibimYbCoMow9ek2SoeQZJ3jtMoDSvWioiZ3t+/gPQ7Wk0HbhORAaLyLnACGARsBgY4TyHTiQzyDxdM29hLvB5d/1YYFqcNBnJkmR3OG5ZTkNDPegxVLK5IAyVlP9SZCUNZbs/OXsGIvIU8GngLBFpAG4HPi0iF5J5jvXAVwBUtVZEngHWAB3Azara6eK5BZgFDAImqWqtu8W3gSkicjewDHg0sdwVgVJ/FMU0bVTLss2KpvNrTQFmSsuftK7plVMZqOr1PuLACltVJwATfOQzgZk+8k1kvI2MlLNoc1+nslKX6aTvb+Mv1UmxynHaFanNQC5zSl0hF4ps2Ur7R1UNVGixi/09FWWHNFu1NN1MXdpQ6iRUFGmo58O5llYnaXg/hSTXe+1/Pt/nEaasFavHGsubyOhl8oItJb1/oVrJfl4VlV4BWo+jOvjaU8v4q/POjHRN0kUjjWXNlIGReko54FapZrhqZvqKHUxfYR7s/TEzkRGavHdqihi+lAvV5bp1Wj1CikW55r/1WHEWpStHTBkYEdB+v8JVCGnsEhvxKPe1eLpSoMRiL4aXaCoGYsqgzElB2S4I2fJV3tWRUa34roQb4gMulv41ZWD4Uiolk4qGZ4Uq2Grm53M28PslDX0q5FKWtTTOabEBZKNopN3OHGlv28IlwygAP52d2c3sqr84O2u4tJfRQmI9AyM0cb+TfFtgSXyeiXzj1VtPVCR+ZaKaX7EpgzInFWaVgpE9c+Em7OSHoqns0hu5aT3WSU193yVUyvp7sRnIRlroXxZLN65Q2C9a0bL3mik05dCCvu25VXz+lwvY1nykR9bW0ZVY/EmYlKJux1pITBmUOeVk4oyS1GrdSSztlJOKXLPjIAAtrR09src37c1+UZjeZhIPIYUP0pSBUXCimFnMJGMkjbdhYaUrGFMGZY5ZM5KjnHpZRm78TH25zH9xe6RBZSfJHm6he8s5lYHb8L5RRFZ7ZGeIyGwR2eD+n+7kIiITRaRORFaKyMWea8a68BtEZKxH/jERWeWumShmrE0t1VRZ5rIHV9Oz8KOc8u9NazkuOFesKjFMz+AxYHQ/2XhgjqqOAOa43wBXktnqcgQwDngIMsqDzA5pnyCzkc3t3QrEhRnnua7/vYyUkG/LJOkKxMYVik85NdXW7jwY+ZpcZTTpeQhpUqo5lYGqvgE09xOPASa748nANR7545rhbTKb3Z8NXAHMVtVmVd0HzAZGu3OnquoCtx/y4564jBC0tHXkDhSD7kK6ff/RvONKugKJE10UxaHq3xpL0XdrxCRuWUy6dZ5GnRp3zOB9qroTwP1/r5MPBbZ5wjU4WTZ5g4/cFxEZJyI1IlLT1NQUM+mVxf4j7QWNvxj7Hhdy1mcS37BfHNYrKU/KqWdTbJIeQPZ71EErAmeT+6Kqj6jqSFUdOWTIkJhJrCzS1M00qoe0LttwrLOLn72ynqPt/ktVJ+mtVuxHUOj7xV2baLeInK2qO52pp9HJG4BzPOGGATuc/NP95K85+TCf8IZRMtJZzaWDtLv+Pr14Gz97ZQPtnsllUSrRNG1D2XO/lK9aOh3o9ggaC0zzyG9wXkWXAgecGWkWMEpETncDx6OAWe5ci4hc6ryIbvDEZYSg0AXFG3+xN7dJNRWVmcqhe/Oa1mPxZhoXe2P7NJkbc/YMROQpMq36s0SkgYxX0D3AMyJyI7AVuNYFnwlcBdQBR4AvA6hqs4jcBSx24e5U1e5B6ctw5NsAABWbSURBVJvIeCydDLzo/oyQFKrrmKY9kAN9uIvy5RbhHmVI2h+Lt/z2KctF3DO8P96GVRrHLnIqA1W9PuDUZ3zCKnBzQDyTgEk+8hrgI7nSYZSGbF4Uha6MS7vevOFLyh9Mvl4/OeeX9L9f2h9IBGwGshGaYg4alnJ8Mq2Do0Z4gl5hOVfdtu2lUVKS/HhKucVfEvV7X1uvkUZ83RPL/GXZqqVGxZBzPZgCfqxJTBbqHpQ81pXc8sdlT0or2GXb9gOw51BbiVMSnSB32GJhyqDMOXD0WEHi7a6g0zjQ1U2h6qP+8S7dmqlgflezbWDgKiPN5QHg+RUZz/RFm3sXTYi2dHphwoah1I0NUwZlTu2O6OuvlBsDBu2KWCF57+11Vyx300OlE/f1hJpnUKBtXEtdpkwZlJhCteyTIpu3RFQf6ahl3Xvnto5Onl+xo2iDu2lvARvZCRxALtCLrYQ9tuPOQDYSYvzvV5Y6CQUn3sJyfbn35fU8/MYm3n1S4Yts8LwG6w50k/4nUZgU9i8CldRosJ5Bidl7qLALzeVLkjOQQ90vQL7zQCuQ/p6UkQ76eH5FKrjFXnDIe1haFWvKwPDFr1iWsrB2K6WuErbOtc9x+tvG1YwGHKeFNE5WM2VQYqxSCUf3pxO3xZeISddeVQ/FNJk1tbQxY+XOSNd401fInc6SpNgb6/THlEGJKacKpnDrIOW+cffAn2rpWlVl9KoqihsmLeLm3y6lpTW8iXDfkd6wUfb8KOnM9wB52lctNRKiZsu+yNf862OLcwdKiFK2pLyeH92Hxf5W+7Yw/VubRmHZvu8IAJ0xN1qatrx3VfwkK9ZKKwOmDMqQV99pzB2oCBTyYxjgteHUUtQxg11u4PmVNbsjXVdJXiLljrdXGIe01dl9GhUB8lJgysDwpadgJro4UYgwAffrqZwjfi+rth8AYEPjoWgXGr6Uor7KVzEn6XQQVGGHm6w2sKebJmyegRGauN9UEgX/uB4zkX+rqpD37rlf2pqYVUa2xz9n7W6Gn3WK/3WeF5drvKkIW34HkuvWhU5aXspAROqBFqAT6FDVkSJyBvA0MByoB76gqvvcTmb3k9n85gjwJVVd6uIZC3zXRXu3qk7OJ11GcqTFBa7XTES6XUKMxOn1JAuuDm+cXBN4LooS//3ShvCBi0ZxCnwSZqK/UdULVXWk+z0emKOqI4A57jfAlcAI9zcOeAjAKY/bgU8AlwC3u60xjSphXwhvj54B5IjNI9Mb5U++S0h4zUS5oorieRSG+JPfik8hxgzGAN0t+8nANR7545rhbeA0ETkbuAKYrarNqroPmA2MLkC6jAh0F9s+M5DzjtM/Bu/m5UFpER8zEcDr65uYuSrYBz3vna/yurpyKeZzaT7cntc9o5h+imEm8pbJvsqi8PfORr7KQIGXRWSJiIxzsve5je5x/9/r5EMB7xrADU4WJDdSTnfZ3Xe4nRVuHXk/cttp/ec7973K36Nk7KRF/PuTS7PcOx5+aY46XmEkiyqs3XmQDbtbIl7nHTPIETZGuuJQiQPIl6nqDhF5LzBbRN7JEtZ3E6Is8oERZBTOOIAPfOADUdNqxCBMmf3CwwvY0HiI+nuujnWPvjNE/e8Yd55Bkh+d7W1TWhTlyvvnAUQqa9GWJooyqz2e6gj0SMoRX6F7Dnn1DFR1h/vfCDxHxua/25l/cP+7neIbgHM8lw8DdmSR+93vEVUdqaojhwwZkk/SjRgEFeJsbpuH2jq4+bfBLXcI5/rXU6dHnPiVlgHwSqMYJo0tew8nYmeP4loaxkzkZ3oMoxhymiwDl93OnaYkiK0MROQUEXl39zEwClgNTAfGumBjgWnueDpwg2S4FDjgzEizgFEicrobOB7lZEYK6GPfjHH9sq25Z1iH+wB70xD0bfzXH1bz2PzN/S7sPVyyZR//+OB82jribS8Y5oNvPdbJ/3joraxmMyM3q7cf4FM/fo1H39ycO3AUEt6YplD7I5SCfMxE7wOecw/jeOC3qvqSiCwGnhGRG4GtwLUu/EwybqV1ZFxLvwygqs0ichfQvcbCnarau2edURIK0fILijNMy+24ELNQn3h7CwBfuuzcHpn3U71t6irW7W5hU9PhnPfLRVCLde3OgyzZso/vTVvNtFs+mfd9qpVtzZklKBbXe6qCmGUyyqBwsReOTNM4VGxloKqbgI/6yPcCn/GRK3BzQFyTgElx02IUjmK0e3KZAjKL02XIZzZpHPfUsN4enV2ZAe+epROiJ8/w4Peu4j7TKNcVa1wojvmy0GXKlqMwfHlx1U5un7aaNTuz7LHcr3R2xfTL87tMta+tNN/1afrEHcq+63ddXxbXN/PWxj0AnHfbTG56convUtvFQlV5fEE9R9o7Cn+vgldNuZXq8m372dSUe5mRYi91HpfArTqLdH9TBoYv46euYvKCLfzLrxf2CnN8KXsOt4WOf7nHph5lE/KoPYOCKRSFa3+5gH/+Ve/zmVW7O3A+RBh2HWjlaHu88QyAuesa+d60WibMWNsje3PDHj4+4ZUBFeKv521izAPzA+NqPtzOD196J3Cl0C6F+2avZ++hNrbuPcKDr9UBsG5XC5N87Px3TK/lP55eHjov3cuPzPYsMNj/vV3zwHwu/+nrOeOat2FP6PuGURx3vbAmdHxR2HPI//uZviLjT7P3UBs/nvVO7EZXLkwZGKH5z1z7NfuUUW932Pudbd7T26Lr7FJaj3VmXa/et1sd4ps4zqMN8m5hBdzvcFtvS9w7tnG0vZMHX6ujozOc7eHSH8zhhkkLUVXufXldz4qrubjsnld5YkE9R5wi8c7o/p+PLqSppY3aHX17eHfPWJt1kPv26bU89NrGzJo/42f0LJs+b0MTkJkoeP+cDYyfuoov/WYRP3opk96rJs7jTp/K8rG36nlu2fZQ+YH4HjvFJOne3+d+/qav/Dfz6wFo2HeUB+ZuZP7G8MotCqYMjJLgrR+7VPmbn7zGX9zxcmD44zw25CgOHH17Br1xxCGoV7J5z8AB6S6Fn81Zz49eWsfUCJXg4vp9rNp+gImv1vG1p5aFumb7/qP817TaHoXpl8yoewE871qjkxfUA5ll02ev2c0tv+2bptlrdvcooS7V2HsO9Md38lECUeey1RfDvOe9RZz7BfUg8sWUgRGb9s4u6hp7Z4PmKtcrGnpbok8t2tp7nfZueB+E10wU5QPyfvz5mHCy0af34alrunsMrceimX6669PWiC6wx2VRdrlyvOdQm6/5YbPH8+o7z63Kft8wifRhzY6DA+7tp2C7lyPvz4Ej4XdAy0XOshFwOtycF89xHt3UX89L2N3WYcrAiM3ybfv57L1v9PzuyNEq9Fb4Szw7vEVyLQXa3FpGi+qjeSBnaznng+9gs2qgO2xX18AW9M4DR3uOp7uduVY2HGD5tv1cPXHeAIXS6ROHV2EeOHqM+1/Z0HPutqmZivzRNzdz78vreuS/emMTew+1MfLuV/iJk3evBQR932nQgoI73Hv1Vui5bO8vrtrJtuYjrGo4wFUT5/HA3Lo+5yfMXDvgmiCzWZRxpFwKthwWQkyq99UfUwZGYlw9cV6s6w4cHdiy69OVpte31PuxNrjtELPhayaKkLYwYfuOSwwco+hfWX3otpmcd9tMAFZs209d46E+i/Vt8oynfP/5Wmp3HBxg87/wzpe57J5X+6WkV2He+fwa7ntlfc+Zbq+wu15Yw8RXeyveCTPXcrA104P53ZIGZtXu4uK7Zvecb2zpNUkcF6E5m6tSvenJpVx1/zy2788owaBWv5fgwexIEwmykuRGOPmwde8Rho+fwZodA7353tkVbW2msJgyMBJjf8zu+tKAWcp+9t3aHb2VRtTlgcOsi5+NoKuCxiW83kvHOruYuWonf/5fL/W5dswD8/nsva/3yesgT4TLtmZMa0u29O0FtbR2sOtg35ay997eQe1cnHRCphpoammjJktvqy3H6rLX/nJBz/HLa3blvG9LWwf9n+r9r2zo8Z7pT1BFHcVbKFdlnw5VABNmZgbhr4rZwIqDKQOjYOw/knFPzEWYXm93y/mFlf7LVQf6aPvU1LEnLwWsi+RVWb3mLO3TE/mXXy/k359cytGA8QNvMo87bqASXLBxb870eRcO8bN97z7ob2Y5/rjeaiBK678/XuX01f+3lE1Nh/jhS+/w33/U24P5wsML/C6lxfVO7ntlfeDAedA7DnqmUeIIez5pgm53uC2+i3FcbNtLo2Bc88B86vfmNuW8/z0n9Rzf8XwtkLFb/90vel3tut3rvNw1o9eF0dviGz5+Bl/51If4zyv+jDfWN/XIu10pu23yAB+6dUbP8Z95Wu3d8T3vaaUu3do7AP7XP57bc+zXM1i/+xBnnjI4k84Av/SvPrGk53i1x0yydMvAntLcdU2oKufeOpP/c8Wf+sbn7YnsOjjQ46TeZ1AWYJvH3Naw/6hvmDgcPdbJQ69t7CNbtLlvz6PbRLVgU25l51VwXtNiWkw7SVKKJY9MGRgFI4wiADjnjHf1HM/wafmv3u4/C9pbB4z47ot9zj38+iaafCpEyPi8dxPUKwk5NQCAMb/onbzlnTSWq4J7qbbXlHKTZ0+GvYf9B2q70/rjWet8z3f3XOa80+h7/p+9Ewg9LKnvVT7rErRHh6mjc21s5MX7TrzeRtHWHspxvkiKJZcCO96nd1hozExklJxcXkhh8Pu2ovj39yfKt3jYowCyzerNF78k7fC05HOZS8IMwA5KsEm6KaAn4iXSukF9zHThPZeC4oiTnsNJLPWhML8ue0NhkCkDoxrptF1jAvnjU3tNaH4VVfeMYIhv7/fqCL/xiri86UlbEngrfW+au8cbwsWR/XwuZdFfn0Z55H5jNkH3y2fsJi6mDIySE8UkU210eiqL5328bLy9qvjKoDeOJM0TQbN9+7TkI7TqveXEG0f/OQrZydEzSMhK1NjSytZ+ZtK73ZpR3jkljUGD+oOKrwxszMAoOdYzCMZr2vmGz0Jv+zzjC3Hr8UPetZWSVAYBUXlb1966N9caTn3MRB75kQiL+0XxJsplfnrsrXo+eGZmvMtrrjvW2cUlE+YA8Nt/+8SA67yOCEEW0lLs0Gc9A6PkJDFmkDRpcVDJNdv0Jy+vz3o+DF6Pn2I0SIMq2X97vCb7dZ7juCt35rrKO9PbW2kfC1BUW1zrf5zHM+ybz6zoOfauauvHrVP9l/iYscrfhbqQpEYZiMhoEVknInUiMr7U6TGKx9OLt5U6CanFb3Z2EK8GeBFFIUkdOKvWf+LZsU7/+Rpz12UfY+iz3EXMNOVS8l5Prls8e3dH2cb07RBusmkkFcpARAYBDwBXAucD14vI+aVNlVEsoswgNYL53ZKGvONIctG3fQFxZVuqPBtdfQaQ4/YMwl/nXUtre4T5F00thVlVtNBIsfxqsyZC5C+BO1T1Cvf7VgBV/UHQNSNHjtSamuzdSj+Gj5+RO5BhGAXjA2e8i63N4eagBPGuEwdFGivo5uQTBkWasZxW1t09msHHD4p1rYgsUdWR/eWp6BkAQwGvraDByfogIuNEpEZEapqa4rmtnXqSjZkbubnigvflHcfwM9/le+zHuwfnLpf/bdh7Bsg+MvRU37AjP3h6zvi6+fjw8GGTwJvm0Rf8cc/xheeclvW6Uef3vpNP/cmQWPf+9J/6X3fmKScOkF30gd70eNPp5fyzBz5/77u+7MNnRk0iAH91XvbrCjHAnJaa0XcviwEC1UeARyDTM4hzo5V3XBHnMsMwjIomLT2DBuAcz+9hgP/ShYZhGEbipEUZLAZGiMi5InIicB0wvcRpMgzDqBpSYSZS1Q4RuQWYBQwCJqlqbYmTZRiGUTWkQhkAqOpMYGap02EYhlGNpMVMZBiGYZQQUwaGYRiGKQPDMAzDlIFhGIZBSpajiIOINAFbYl5+FlANC+JUSz7B8lqJVEs+obh5/aCqDpiKXbbKIB9EpMZvbY5Ko1ryCZbXSqRa8gnpyKuZiQzDMAxTBoZhGEb1KoNHSp2AIlEt+QTLayVSLfmEFOS1KscMDMMwjL5Ua8/AMAzD8GDKwDAMw6guZSAio0VknYjUicj4UqcnLiJSLyKrRGS5iNQ42RkiMltENrj/pzu5iMhEl+eVInKxJ56xLvwGERlbqvx4EZFJItIoIqs9ssTyJiIfc8+uzl2b/JZRIQjI5x0ist291+UicpXn3K0uzetE5AqP3LdMu+XgF7r8P+2Whi86InKOiMwVkbUiUisiX3fySnynQXktj/eqqlXxR2Zp7I3Ah4ATgRXA+aVOV8y81ANn9ZP9CBjvjscDP3THVwEvktlN7lJgoZOfAWxy/093x6enIG9/DVwMrC5E3oBFwF+6a14ErkxRPu8A/rdP2PNdeR0MnOvK8aBsZRp4BrjOHf8SuKlE+TwbuNgdvxtY7/JTie80KK9l8V6rqWdwCVCnqptUtR2YAowpcZqSZAww2R1PBq7xyB/XDG8Dp4nI2cAVwGxVbVbVfcBsYHSxE90fVX0DaO4nTiRv7typqrpAM1/T4564ikpAPoMYA0xR1TZV3QzUkSnPvmXatYwvB55113ufWVFR1Z2qutQdtwBryexvXonvNCivQaTqvVaTMhgKbPP8biD7i0ozCrwsIktEZJyTvU9Vd0KmUALvdfKgfJfT80gqb0PdcX95mrjFmUcmdZtOiJ7PM4H9qtrRT15SRGQ4cBGwkAp/p/3yCmXwXqtJGfjZEcvVr/YyVb0YuBK4WUT+OkvYoHxXwvOImre05/kh4DzgQmAn8FMnL/t8isgfAb8HvqGqB7MF9ZGVe17L4r1WkzJoAM7x/B4G7ChRWvJCVXe4/43Ac2S6lbtdlxn3v9EFD8p3OT2PpPLW4I77y1OBqu5W1U5V7QJ+Rea9QvR87iFjXjm+n7wkiMgJZCrHJ1V1qhNX5Dv1y2u5vNdqUgaLgRFuNP5E4DpgeonTFBkROUVE3t19DIwCVpPJS7eHxVhgmjueDtzgvDQuBQ64bvksYJSInO66raOcLI0kkjd3rkVELnX21xs8cZWc7srR8Q9k3itk8nmdiAwWkXOBEWQGTX3LtLOdzwU+7673PrOi4p7zo8BaVb3Xc6ri3mlQXsvmvRZrpD0Nf2Q8FdaTGan/TqnTEzMPHyLjXbACqO3OBxl74hxgg/t/hpML8IDL8ypgpCeufyUzaFUHfLnUeXNpeopMV/oYmRbSjUnmDRhJ5mPcCPwCNws/Jfl8wuVjJZmK4mxP+O+4NK/D4y0TVKZdOVnk8v87YHCJ8vlJMqaMlcBy93dVhb7ToLyWxXu15SgMwzCMqjITGYZhGAGYMjAMwzBMGRiGYRimDAzDMAxMGRiGYRiYMjAMwzAwZWAYhmEA/x9weWrQd9Tx8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(demand)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7c8ae36d97c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0melectric_consumption\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_electric_consumption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_electric_consumption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "electric_consumption = []\n",
    "for i in range(len(env.total_electric_consumption)-1):\n",
    "    env.total_electric_consumption[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'action_track'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-08cea7ab34a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0muid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuildings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_track\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_track\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'action_track'"
     ]
    }
   ],
   "source": [
    "for uid in buildings:\n",
    "    plt.plot(range(len(env.action_track[uid])), env.action_track[uid])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
