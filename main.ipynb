{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment\n",
    "data_folder = Path(\"data/\")\n",
    "building_attributes = data_folder / 'building_attributes.json'\n",
    "solar_profile = data_folder / 'solar_generation_1kW.csv'\n",
    "building_state_actions = 'buildings_state_action_space.json'\n",
    "building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
    "env = CityLearn(building_attributes, solar_profile, building_ids, buildings_states_actions = building_state_actions, cost_function = ['quadratic'])\n",
    "observations_spaces,actions_spaces = env.get_state_action_spaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL CONTROLLER\n",
    "from reward_function import reward_function\n",
    "from agent import TD3_Agents\n",
    "import time\n",
    "\n",
    "#Instantiatiing the control agent(s)\n",
    "agents = TD3_Agents(observations_spaces,actions_spaces)\n",
    "\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "'''\n",
    "Every episode runs the RL controller for over 2500 hours in the simulation (controls the storage of cooling energy over about 3 Summer months)\n",
    "Multiple episodes will run the RL controller over the same period over and over again, improving the policy and maximizing the score (less negative scores are better)\n",
    "CHALLENGE:\n",
    "Running the RL algorithm to coordinate as many buildings as possible (at least 3 or 4) and find a good control policy in the minimum number of episodes as possible (ideally within a single episode)\n",
    "'''\n",
    "start = time.time()\n",
    "episodes = 7\n",
    "for e in range(episodes): #A stopping criterion can be added, which is based on whether the cost has reached some specific threshold or is no longer improving\n",
    "    cum_reward[e] = 0\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if k%500==0:\n",
    "            print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "            \n",
    "        action = agents.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward_function(reward) #See comments in reward_function.py\n",
    "        agents.add_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cum_reward[e] += reward[0]\n",
    "        rewards.append(reward)\n",
    "        k+=1\n",
    "    cost[e] = env.cost()\n",
    "end = time.time()\n",
    "print((end-start)/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the cost of every episode. The lower the better. Ideally it will reach the same cost or lower than the RBC: 156.88\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the actor and critic losses\n",
    "fig, (plot1, plot2, plot3) = plt.subplots(1,3)\n",
    "fig.set_size_inches(12,4)\n",
    "plot1.plot(agents.critic1_loss_list[0],'b')\n",
    "plot2.plot(agents.critic2_loss_list[0],'g')\n",
    "plot3.plot(agents.actor_loss_list[0],'y')\n",
    "plot1.set_xlabel('hours*iterations')\n",
    "plot1.set_ylabel('Critic 1 loss')\n",
    "plot2.set_xlabel('hours*iterations')\n",
    "plot2.set_ylabel('Critic 2 loss')\n",
    "plot3.set_xlabel('hours*iterations')\n",
    "plot3.set_ylabel('Actor loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the actor and critic losses\n",
    "fig, (plot1, plot2, plot3) = plt.subplots(1,3)\n",
    "fig.set_size_inches(12,4)\n",
    "plot1.plot(agents.critic1_loss_list[0],'b')\n",
    "plot2.plot(agents.critic2_loss_list[0],'g')\n",
    "plot3.plot(agents.actor_loss_list[0],'y')\n",
    "plot1.set_xlabel('hours*iterations')\n",
    "plot1.set_ylabel('Critic 1 loss')\n",
    "plot2.set_xlabel('hours*iterations')\n",
    "plot2.set_ylabel('Critic 2 loss')\n",
    "plot3.set_xlabel('hours*iterations')\n",
    "plot3.set_ylabel('Actor loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the actor and critic losses\n",
    "fig, (plot1, plot2, plot3) = plt.subplots(1,3)\n",
    "fig.set_size_inches(12,4)\n",
    "plot1.plot(agents.critic1_loss_list[0],'b')\n",
    "plot2.plot(agents.critic2_loss_list[0],'g')\n",
    "plot3.plot(agents.actor_loss_list[0],'y')\n",
    "plot1.set_xlabel('hours*iterations')\n",
    "plot1.set_ylabel('Critic 1 loss')\n",
    "plot2.set_xlabel('hours*iterations')\n",
    "plot2.set_ylabel('Critic 2 loss')\n",
    "plot3.set_xlabel('hours*iterations')\n",
    "plot3.set_ylabel('Actor loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the actor and critic losses\n",
    "fig, (plot1, plot2, plot3) = plt.subplots(1,3)\n",
    "fig.set_size_inches(12,4)\n",
    "plot1.plot(agents.critic1_loss_list[0],'b')\n",
    "plot2.plot(agents.critic2_loss_list[0],'g')\n",
    "plot3.plot(agents.actor_loss_list[0],'y')\n",
    "plot1.set_xlabel('hours*iterations')\n",
    "plot1.set_ylabel('Critic 1 loss')\n",
    "plot2.set_xlabel('hours*iterations')\n",
    "plot2.set_ylabel('Critic 2 loss')\n",
    "plot3.set_xlabel('hours*iterations')\n",
    "plot3.set_ylabel('Actor loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for the last 100 hours of the simulation\n",
    "plt.plot(env.buildings[0].cooling_device.electrical_consumption_cooling[2400:])\n",
    "plt.plot(env.buildings[0].sim_results['cooling_demand'][3500:6000].values[2400:])\n",
    "plt.plot(env.buildings[0].cooling_device.cooling_supply[2400:])\n",
    "plt.legend(['Electrical consumption cooling','Building cooling demand','Heat pump cooling supply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for the last 100 hours of the simulation\n",
    "plt.plot(env.buildings[0].cooling_storage.soc_list[2400:])\n",
    "plt.plot(env.buildings[0].cooling_storage.energy_balance_list[2400:])\n",
    "plt.legend(['State of Charge','Storage device energy balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for the last 100 hours of the simulation\n",
    "plt.plot(env.action_track[8][-100:])\n",
    "plt.plot(env.buildings[0].cooling_device.cop_cooling_list[-100:])\n",
    "plt.legend(['RL Action','Heat Pump COP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the RL algorithm reaches convergence this plot should show the heat pump loading the energy storage device then the COP is high and releasing the energy when the COP is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots all the actions across episodes (includes exploration noise and environment constraints)\n",
    "plt.plot(env.action_track[8][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agents.a_track1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agents.a_track2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
